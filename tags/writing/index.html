<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Writing</title>
<meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Writing</div></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><main><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhaml_dipping_into_pytorch/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhaml_dipping_into_pytorch/>AML: Dipping into PyTorch</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.</p><h2 id=getting-started>Getting Started</h2><p>To get started, let&rsquo;s open a <a href=https://colab.research.google.com/>colab</a> and import Torch!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span></code></pre></div><p>The top line here import PyTorch generally, and the bottom line imports the Neural Network libraries. We will need both for today and into the future!</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhaml_it_takes_two/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhaml_it_takes_two/>AML: It Takes Two</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>Hello everyone! It&rsquo;s April, which means we are ready again for a new unit. Let&rsquo;s dive in.</p><p>You know what&rsquo;s better than one neural network? TWO!!! Multi-modal approaches&mdash;making two neural networks interact for a certain result&mdash;dominate many of the current edge of neural network research. In this unit, we are going to introduce one such approach, <strong>Generative Adversarial Networks</strong> (<strong>GAN</strong>), but leave you with some food for thought for other possibilities for what training multiple networks together can do.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhaml_reinforce/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhaml_reinforce/>AML: REINFORCE(ment learning)</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>Woof. As I begin to write this I should add that <strong>this unit is going to be conceptually dense</strong>. Though we are teaching one particular algorithm (incidentally, named, REINFORCE), the world of reinforcement learning is build by one, if not many, very advanced treatments in maths.</p><p>So if anything, I would focus on getting the conceptual flavor of how these problems are formulated and discuses. If you can be along for the mathematical and algorithmic journey, then even better &mdash; but by no means required or expected&mldr; There&rsquo;s still lots for all of us to learn together.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhaml_time_to_convolve/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhaml_time_to_convolve/>AML: Time to Convolve</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>Welcome back! I think, over the last few days, we have been hyping up convolutional neural networks enough such that you are probably ready to dive right in. So&mldr; Let&rsquo;s, uh, motivate it first!</p><h2 id=why-do-we-use-a-cnn>Why do we use a CNN?</h2><p>Let&rsquo;s think of a toy problem to play with. Given a pattern made using two colours (let&rsquo;s name them a and b, or perhaps black and white), let&rsquo;s classify whether it is the &ldquo;zebra&rdquo; pattern" or the &ldquo;checkerboard&rdquo; pattern.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhllms_are_text_matchers/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhllms_are_text_matchers/>LLMs are fantastic search engines, so I built one</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>For the past 20 years, semantic indexing sucked.</p><p>For the most part, the core offerings of search products in the last while is divided into two categories:</p><ol><li>Full-text search things (i.e. every app in the face of the planet that stores text), which for the most part use something n-grammy like <a href=https://en.wikipedia.org/wiki/Okapi_BM25>Okapi BM25</a> to do nice fuzzy string matching</li><li>Ranking/Recommendation things, who isn&rsquo;t so much trying to <em>search</em> a database as they are trying to guess the user&rsquo;s intent and <em>recommend</em> them things from it</li></ol><p>And we lived in a pretty happy world in which, depending on the application, developers chose one or the other to build.</p></span></div><ul class="pagination pagination-default"><li class="page-item disabled"><a aria-disabled=true aria-label=First class=page-link role=button tabindex=-1><span aria-hidden=true>⟸</span></a></li><li class="page-item disabled"><a aria-disabled=true aria-label=Previous class=page-link role=button tabindex=-1><span aria-hidden=true>⟵</span></a></li><li class="page-item active"><a aria-current=page aria-label="Page 1" class=page-link role=button>1</a></li><li class=page-item><a href=/tags/writing/page/2/ aria-label="Page 2" class=page-link role=button>2</a></li><li class=page-item><a href=/tags/writing/page/2/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/tags/writing/page/2/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
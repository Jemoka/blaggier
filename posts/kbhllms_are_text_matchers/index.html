<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>LLMs are fantastic search engines, so I built one</title>
<meta name=description content="For the past 20 years, semantic indexing sucked.
For the most part, the core offerings of search products in the last while is divided into two categories:

Full-text search things (i.e. every app in the face of the planet that stores text), which for the most part use something n-grammy like Okapi BM25 to do nice fuzzy string matching
Ranking/Recommendation things, who isn&rsquo;t so much trying to search a database as they are trying to guess the user&rsquo;s intent and recommend them things from it

And we lived in a pretty happy world in which, depending on the application, developers chose one or the other to build."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>LLMs are fantastic search engines, so I built one</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p>For the past 20 years, semantic indexing sucked.</p><p>For the most part, the core offerings of search products in the last while is divided into two categories:</p><ol><li>Full-text search things (i.e. every app in the face of the planet that stores text), which for the most part use something n-grammy like <a href=https://en.wikipedia.org/wiki/Okapi_BM25>Okapi BM25</a> to do nice fuzzy string matching</li><li>Ranking/Recommendation things, who isn&rsquo;t so much trying to <em>search</em> a database as they are trying to guess the user&rsquo;s intent and <em>recommend</em> them things from it</li></ol><p>And we lived in a pretty happy world in which, depending on the application, developers chose one or the other to build.</p><p>There&rsquo;s something really funny to do with this idea of &ldquo;search&rdquo;. Take, for instance, Google. Its a &ldquo;search&rdquo; engine&mdash;but really it <em>recommends</em> people information that is probably relevant; PageRank, the company&rsquo;s claim to fame, isn&rsquo;t even textual analysis of any type at all: it is a measure of <em>relevance</em>, based on centrality arguments about where the average web surfer may end up.</p><p>By framing systems like Google as an act of recommendation, we can see why it is so widely adopted: it, really, brings the best of the Internet to the user&mdash;a catalogue of sorts&mdash;based on text which the user provides as input data regarding their interest. It is, importantly, <em>not a capital-s Search engine</em>.</p><p>And perhaps this explains why this doesn&rsquo;t work:</p><figure><img src=/ox-hugo/2023-09-02_15-01-29_screenshot.png alt="Figure 1: Oh my lord scary books." width=60% height=60%><figcaption><p><span class=figure-number>Figure 1: </span>Oh my lord scary books.</p></figcaption></figure><p>Wouldn&rsquo;t it be nice for a query like this to return us actual, non-scary books?</p><p>If you think about it, back in the olden days (i.e. 2019), there really isn&rsquo;t a way to reconcile this difference between search and recommendation engines. Textual-based search systems were fantastically fast and gave you the exact things you needed&mdash;great for filing &ldquo;that file named this&rdquo;&mdash;but categorically useless when it comes to parsing large amounts of data that the user doesn&rsquo;t know the exact terminology for.</p><p>Recommendation engines, on the other hand, often required special indexing or behavioral modeling for what &ldquo;an average user of this type&rdquo; would like, which is great for leading the user to discover certain things they wouldn&rsquo;t otherwise find, but makes hilarious mistakes like the above because, well, they aren&rsquo;t doing much linguistic analysis.</p><p>So, what if we can simultaneously do the guessing game for user behavior&mdash;a la &ldquo;recommendation engines&rdquo;&mdash;but still use a fundamentally text-based approach to perform searching&mdash;a la a &ldquo;search service&rdquo;?</p><h2 id=llms-are-text-matchers>LLMs are text matchers</h2><h3 id=transformers-are-text-matchers>Transformers are Text Matchers</h3><p>Fundamentally, the <a href=https://arxiv.org/abs/1706.03762>Transformer</a> (the basis of all of those lovely large-language models (LLMs)) is a &ldquo;sequence transduction model&rdquo;&mdash;they are (and were originally invented as) a <em>translation</em> model of sorts. And I find it easy and productive to think of LLMs in that mindframe: although their output may look like human-like &ldquo;reasoning&rdquo;, LLMs&rsquo; fundamental job is to <em>match</em> one bit of text (context) against another (output).</p><h3 id=llms-are-internet-text-matchers>LLMs are Internet Text Matchers</h3><p>The actual thing that is making the whole of the world go crazy right now, though, is the fact that LLMs, transformers trained on the internet, seem to be able to handle text-to-text &ldquo;translation&rdquo; tasks of a <em>much</em> more general nature:</p><ul><li>&ldquo;given a food, <strong>translate</strong> it into its recipe&rdquo;</li><li>&ldquo;given this text, <strong>translate</strong> it into the same written with a pirate voice&rdquo;</li><li>&ldquo;given the comment, <strong>translate</strong> it to the code that follows&rdquo;</li></ul><p>You see, instead of carefully supervised translations a la the original Transformer, GPT+friends is simply chopping off the entirety of the input encoding process and letting the decoder ramble on its own. Hence, its outputs are functionally text matches of the context against the <strong>training data</strong>: that is, these LLM models effectively are text-matchers between the whole of the <em>internet</em> and your input.</p><h3 id=and-hence-the-promise>And hence, the promise</h3><p>Let&rsquo;s recap real quick.</p><ol><li>Transformers are text matchers</li><li>LLMs, which are transformers, are text matchers against the text on the internet</li></ol><p>And here&rsquo;s the corollary to both of these statements:</p><ol><li>Transformers match text</li><li>LLMs, which are transformers, is trained (and biased) towards the things people would typically be looking for on the internet</li></ol><p>That is, though LLMs fundamentally match text, they know what&rsquo;s on the internet and how people talk about (and try to look for) them.</p><p>The point above is, to me, a very profound idea. LLMs essentially achieve what we were hoping to edge towards in the last 20 years&mdash;closing the gap between <em>recommendation</em> (what people want) and <em>search</em> (text matching) systems into one uniform interface.</p><p>And, praise be, LLMs seems to be highly directable at this task as well: they excel at <a href=https://arxiv.org/abs/2005.14165>few-shot and zero-shot training tasks</a>; meaning, if you just give the Transformer a few examples of how to &ldquo;translate&rdquo; a piece of its input, it will happily do it with some accuracy following your example.</p><h2 id=aside-stop-building-gosh-darn-chatbots>Aside: Stop Building Gosh Darn Chatbots</h2><p>Fortunately, people not named yours truly has also noticed these exciting capabilities of LLMs.</p><p>What confuses me, though, is the fact that everybody and their pet duck is building a chat bot or &ldquo;answering&rdquo; service of some description: capitalizing on the fact that LLMs have trained knowledge of text on the internet, but completely disregarding the fact that LLMs fundamentally are best at &ldquo;matching&rdquo; existing text in its context and <em>not</em> hallucinating new text&mdash;as these &ldquo;answer services&rdquo; want to do.</p><p>What gives? <a href=https://wattenberger.com/thoughts/boo-chatbots>Wattenburger has this fantastic take</a> on why chat bots are not the best interface for LLMs. To me, the most salient observation&mdash;one which stems from her wonderful arguments about chat bot&rsquo;s poor observability and iteration cycle&mdash;is that the generated text from these current LLM &ldquo;search&rdquo; services (called &ldquo;retrial augmented generation&rdquo;) is just <em>so darn long</em>.</p><p>When we look information on a site like Google, I believe our goal is generally to shove the desired information in our head as quickly as possible and know where we can go to learn more; if we wanted to read a 300 word summary about it (as Perplexity AI, Mendable, Phind etc. is want to give us) we can just go look up the source ourselves.</p><p>To me, the duty of a search platform, LLM or not, is to get the user on their merry way as quickly as possible&mdash;information in head or link in hand&mdash;not for the user to navigate a possibly-hallucinated rant about the topic they are looking for, followed by 3 source citations.</p><h2 id=making-a-llm-search-engine>Making a LLM Search Engine</h2><p>And so we face a rather daunting task. To make a better search service with LLMs, we have to:</p><ol><li>Leverage LLM&rsquo;s fantastic text matching capabilities</li><li>Allow LLMs to inject their trained biases into what&rsquo;s relevant to the user in order to act as a good recommendation engine</li><li>Do so in as little words as possible written by the LLM</li></ol><p>These three bullet points has consumed much of my life for the past 6 months, culminating in a reference implementation of such a &ldquo;LLM search engine&rdquo; called <a href=https://github.com/Shabang-Systems/simon>Simon</a>. Let me now tell you its story.</p><h3 id=fulfilling-a-search-query-part-1-3>Fulfilling a search query, Part 1/3</h3><p>Our first goal is to figure out</p><h2 id=side-quest-actual-text-to-text-recommendation>Side quest: Actual Text-to-text Recommendation</h2><h2 id=now-you-try>Now You Try</h2></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
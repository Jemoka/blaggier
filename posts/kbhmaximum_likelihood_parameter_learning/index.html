<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Maximum Likelihood Parameter Learning</title>
<meta name=description content="&ldquo;We find the parameter that maximizes the likelihood.&rdquo;

for each \(X_{j}\), sum

what&rsquo;s the log-likelihood of one \(X_{i}\)


take derivative w.r.t. \(\theta\) and set to \(0\)
solve for \(\theta\)

(this maximizes the log-likelihood of the data!)
that is:
\begin{equation}
\theta_{MLE} = \arg\max_{\theta} P(x_1, \dots, x_{n}|\theta) = \arg\max_{\theta} \qty(\sum_{i=1}^{n} \log(f(x_{i}|\theta))  )
\end{equation}

If your \(\theta\) is a vector of more than \(1\) thing, take the gradient (i.e. partial derivative against each of your variables) of the thing and solve the place where the gradient is identically \(0\) (each slot is \(0\)). That is, we want:"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Maximum Likelihood Parameter Learning</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p>&ldquo;We find the <a href=/posts/kbhparameter/>parameter</a> that maximizes the likelihood.&rdquo;</p><ol><li>for each \(X_{j}\), sum<ol><li>what&rsquo;s the <a href=#log-likelihood>log-likelihood</a> of one \(X_{i}\)</li></ol></li><li>take derivative w.r.t. \(\theta\) and set to \(0\)</li><li>solve for \(\theta\)</li></ol><p>(this maximizes the <a href=#log-likelihood>log-likelihood</a> of the data!)</p><p>that is:</p><p>\begin{equation}
\theta_{MLE} = \arg\max_{\theta} P(x_1, \dots, x_{n}|\theta) = \arg\max_{\theta} \qty(\sum_{i=1}^{n} \log(f(x_{i}|\theta)) )
\end{equation}</p><hr><p>If your \(\theta\) is a vector of more than \(1\) thing, take the gradient (i.e. partial derivative against each of your variables) of the thing and solve the place where the gradient is identically \(0\) (each slot is \(0\)). That is, we want:</p><p>\begin{equation}
\mqty[\pdv{LL(\theta)}{\theta_{1}} \\ \pdv{LL(\theta)}{\theta_{2}} \\ \pdv{LL(\theta)}{\theta_{3}} \\ \dots] = \mqty[0 \\ 0 \\0]
\end{equation}</p><ul><li><a href=/posts/kbhprobability_of_k_in_x_time/#mle-for-id-58a7600a-5169-4473-8ddc-f286534fc1f4-poisson-distribution>MLE for poisson distribution</a></li><li><a href=/posts/kbhbernoulli_random_variable/#mle-for-bernouli>MLE for Bernouli</a></li><li><a href=/posts/kbhmle_for_gaussian/>MLE for Gaussian</a></li></ul><p>MLE is REALLY bad at generalizing to unseen data. Hence why MLE is good for big data where your MLE slowly converge to best parameters for your actual dataset.</p><hr><p>We desire \(\theta\) parameter from some data \(D\). To do this, we simply optimize:</p><p>\begin{equation}
\hat{\theta} = \arg\max_{\theta}P(D|\theta)
\end{equation}</p><p>, where:</p><p>\begin{equation}
P(D|\theta) = \prod_{i} P(o_{i}| \theta)
\end{equation}</p><p>for each \(o_{i} \in D\). and \(P\) is the <a href=/posts/kbhlikelyhood/>likelyhood</a>: <a href=/posts/kbhprobability_mass_function/>PMF</a> or <a href=/posts/kbhprobability_density_function/>PDF</a> given what you are working with.</p><p>That is, we want the parameter \(\theta\) which maximizes the likelyhood of the data. This only works, of course, if each \(o_{i} \in D\) is <a href=/posts/kbhprobability/#independence>independent</a> from each other, which we can assume so by calling the samples from data <a href=/posts/kbhindependently_and_identically_distributed/>IID</a> (because they are independent draws from the underlying distribution.)</p><h2 id=log-likelihood>log-likelihood</h2><p>The summation above is a little unwieldy, so we take the logs and apply log laws to turn the multiplication into a summation:</p><p>\begin{equation}
\hat{\theta} = \arg\max_{\theta} \sum_{i} \log P(o_{i}|\theta)
\end{equation}</p><p>&ldquo;add the log probabilities of each of the outcomes you observed happening according to your unoptimized theta, and maximize it&rdquo;</p><h3 id=argmax-of-log>argmax of log</h3><p>This holds because <a href=/posts/kbhlog_laws/>log</a> is monotonic (&ldquo;any larger input to a log will lead to a larger value&rdquo;):</p><p>\begin{equation}
\arg\max_{x} f(x) = \arg\max_{x} \log f(x)
\end{equation}</p><h3 id=mle-in-general>MLE, in general</h3><p>\begin{equation}
\theta_{MLE} = \arg\max_{\theta} \qty(\sum_{i=1}^{n} \log(f(x_{i}|\theta)) )
\end{equation}</p><h2 id=example>Example</h2><p>Say we want to train a model to predict whether or not a plane will crash. Suppose our network is very simple:</p><p>\(\theta\) represents if there will be an midair collision. Therefore, we have two disconnected nodes:</p><p>\begin{equation}
P(crash) = \theta
\end{equation}</p><p>\begin{equation}
P(safe) = 1-\theta
\end{equation}</p><p>Now, suppose we observed that there was \(m\) flights and \(n\) midair collisions between them. We can then write then:</p><p>\begin{equation}
P(D|\theta) = \theta^{n}(1-\theta)^{m-n}
\end{equation}</p><p>because \(\theta^{n}(1-\theta)^{m-n}\) is the total probability of the data you are given occurring (\(n\) crashes, \(m-n\) non crashing flights).</p><p>Now, we seek to maximise this value&mdash;because the probability of \(P(D)\) occurring should be \(1\) because \(D\) actually occured.</p><p>Its mostly algebra at this point:</p><figure><img src=/ox-hugo/2023-10-05_10-07-18_screenshot.png></figure><p>Steps:</p><ol><li>we first compute the probability of each of the sample happening according to old \(\theta\) to get \(P(D|\theta)\)</li><li>we then take the log of it to make it a summation</li><li>we then try to maximize \(\theta\) to</li></ol><p>What this tells us is&mldr;</p><h2 id=generic-maximum-likelihood-estimate>Generic Maximum Likelihood Estimate</h2><p>Overall, its kind of unsurprising from the <a href=/posts/kbhprobability/#frequentist-definition-of-probability>Frequentist Definition of Probability</a>, but:</p><p>\begin{equation}
\hat{\theta}_{i} = \frac{n_{i}}{\sum_{j=1}^{k} n_{j}}
\end{equation}</p><p>for some observations \(n_{1:k}\).</p><p>and:</p><p>\begin{equation}
\sigma^{2} = \frac{\sum_{}^{} (o_{i} - \hat{u})^{2}}{m}
\end{equation}</p><h2 id=problems-with-maximum-likelihood-parameter-learning--kbhmaximum-likelihood-parameter-learning-dot-md>Problems with <a href=/posts/kbhmaximum_likelihood_parameter_learning/>Maximum Likelihood Parameter Learning</a></h2><p>This requires a lot of data to make work: for instance&mdash;if we don&rsquo;t have any plane crashes observed in \(n\) files, this scheme would say there&rsquo;s no chance of plane crashes. This is not explicitly true.</p><p>Therefore, we use <a href=/posts/kbhbaysian_parameter_learning/>Baysian Parameter Learning</a>.</p></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Pretraining Data</title>
<meta name=description content="Problems of pre-training data

pre-training influence downstream capabilities
&mldr;and therefore can escape into model generation
real world users expect novelty

Changes in Distribution
Big Pretraining Data
GPT2

deduplicated data
Removed Wikipedia (to prevent data leak)
Heuristic based cleaning

GPT3

Deduplicated
based on leaked data

Llama
the usual spheal

removed high perplexity data using wiki n-gram model
removed non-English
deduplicated

Llama 2

removed high volue of PII
Removed non-english

Pretraining Curation Decisions

what to include
what is the timestamp being scraped
heuristic based cleaning? data cleaning? etc.
language filtering (only take English?)
PII removal
dedup
Toxicity + SafeURL filtering
&ldquo;quality filtering&rdquo;
sampling distributions

Change in Model Age
Good alignment shown between validation year and pre-training year, even mixing in older data."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Pretraining Data</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><h2 id=problems-of-pre-training-data>Problems of pre-training data</h2><ol><li>pre-training influence downstream capabilities</li><li>&mldr;and therefore can escape into model generation</li><li>real world users expect novelty</li></ol><h2 id=changes-in-distribution>Changes in Distribution</h2><h3 id=big-pretraining-data>Big Pretraining Data</h3><h4 id=gpt2>GPT2</h4><ul><li>deduplicated data</li><li>Removed Wikipedia (to prevent data leak)</li><li>Heuristic based cleaning</li></ul><h4 id=gpt3>GPT3</h4><ul><li>Deduplicated</li><li>based on leaked data</li></ul><h4 id=llama>Llama</h4><p>the usual spheal</p><ul><li>removed high perplexity data using wiki n-gram model</li><li>removed non-English</li><li>deduplicated</li></ul><h4 id=llama-2>Llama 2</h4><ul><li>removed high volue of PII</li><li>Removed non-english</li></ul><h3 id=pretraining-curation-decisions>Pretraining Curation Decisions</h3><ul><li>what to include</li><li>what is the timestamp being scraped</li><li>heuristic based cleaning? data cleaning? etc.</li><li>language filtering (only take English?)</li><li>PII removal</li><li>dedup</li><li>Toxicity + SafeURL filtering</li><li>&ldquo;quality filtering&rdquo;</li><li>sampling distributions</li></ul><h3 id=change-in-model-age>Change in Model Age</h3><p>Good alignment shown between validation year and pre-training year, even mixing in older data.</p><p>Implication: &ldquo;fine-tuned T5 may still be worse than fine-tuned llama, because T5 was <strong>pretrained</strong> using older data&mdash;despite even if FTing is newer&rdquo;</p><h3 id=change-in-toxicity>Change in Toxicity</h3><p>Filtering toxicity made the model worst at spotting toxicity.</p><h3 id=change-in-data-distribution>Change in Data Distribution</h3><p>out of domain answers do worse on out of domain results</p><h2 id=reduce-memorization>Reduce Memorization</h2><ol><li>de-duplication using <strong>approximate matching</strong></li><li>think carefully for multiple-epoch training (what is ok to memorize?)</li><li>remove sensitive memorization from pre-training data</li></ol><p>Two iffy strategies:</p><h3 id=check-for-memorization>Check for memorization</h3><p>Trivial style transfers can get around safety checks &ldquo;do the [copyrighted thing] in French&rdquo;; &ldquo;do the [copyrighted thing] with double the spaces&rdquo;.</p><h3 id=use-rlhf-or-something>Use RLHF or something</h3><p>&ldquo;hide flaws, and not eliminate them&rdquo;&mdash;edge case problems doesn&rsquo;t eliminate the underlying vulnerability.</p></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>conditional plan</title>
<meta name=description content="conditional plan is a POMDP representation technique. We can represent a conditional plan as a tree.
toy problem
crying baby POMDP problem:

actions: feed, ignore
reward: if hungry, negative reward
state: two states: is the baby hungry or not
observation: noisy crying (she maybe crying because she&rsquo;s genuinely hungry or crying just for kicks)

formulate a conditional plan
we can create a conditional plan by generating a exponential tree based on the observations. This is a policy which tells you what you should do given the sequence of observations you get, with no knowledge of the underlying state."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>conditional plan</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p><a href=/posts/kbhconditional_plan/>conditional plan</a> is a <a href=/posts/kbhpartially_observable_markov_decision_process/>POMDP</a> representation technique. We can represent a <a href=/posts/kbhconditional_plan/>conditional plan</a> as a tree.</p><h2 id=toy-problem>toy problem</h2><p>crying baby <a href=/posts/kbhpartially_observable_markov_decision_process/>POMDP</a> problem:</p><ul><li><strong>actions</strong>: feed, ignore</li><li><strong>reward</strong>: if hungry, negative reward</li><li><strong>state</strong>: two states: is the baby hungry or not</li><li><strong>observation</strong>: noisy crying (she maybe crying because she&rsquo;s genuinely hungry or crying just for kicks)</li></ul><h2 id=formulate-a-conditional-plan--kbhconditional-plan-dot-md>formulate a <a href=/posts/kbhconditional_plan/>conditional plan</a></h2><p>we can create a <a href=/posts/kbhconditional_plan/>conditional plan</a> by generating a exponential tree based on the <strong>observations</strong>. This is a <a href=/posts/kbhpolicy/>policy</a> which tells you what you should do given the sequence of observations you get, with no knowledge of the underlying state.</p><figure><img src=/ox-hugo/2023-11-14_10-04-21_screenshot.png></figure><p>We call this plan \(\pi\) (shock suprise). We define two notations:</p><ul><li>\(\pi()\): the <strong><strong>ACTION</strong></strong> at the head of this tree (in this case, &ldquo;ignore&rdquo;)</li><li>\(\pi(o)\): the <strong><strong>SUBTREE</strong></strong> which is one-level below the first action. For instance, for both observations of the tree above, \(\pi(o)()\) is ignore for both \(o\).</li></ul><h2 id=conditional-plan--kbhconditional-plan-dot-md--evaluation><a href=/posts/kbhconditional_plan/>conditional plan</a> evaluation</h2><p>Assume we have a starting at some given true state \(s\). We can evaluate a <a href=/posts/kbhconditional_plan/>conditional plan</a> at that state by formulating:</p><p>\begin{equation}
U^{\pi} (s) = R(s, \pi()) + \gamma \qty[\sum_{s&rsquo;} T(s&rsquo;|s, \pi()) \sum_{o} O(o|\pi(), s&rsquo;) U^{\pi(o)}(s&rsquo;)]
\end{equation}</p><p>where, \(\pi()\) is the action at the root node of the tree; and \(\pi(o)\) is the subtree for subplan at observation \(o\); essentially, at each point where we evaluate \(U\), we move the root node forward and recalculate. If we run out of depth, the utility is \(0\) and hence the whole right term is \(0\).</p><p>Of course this assumes we know what our initial state is. Which is lame. So now:</p><p>\begin{equation}
U^{\pi}(b) = \sum_{s}^{} b(s) U^{\pi}(s)
\end{equation}</p><p>which will give us the <a href=/posts/kbhutility_theory/>utility</a> of our policy given a <a href=/posts/kbhbelief/>belief</a> about wher ewe are.</p><p>so literally take our belief about the probability of us being in each initial state and calculate it for each of our initial states.</p><h2 id=optimal-value-function--kbhpolicy-dot-md--for-pomdp--kbhpartially-observable-markov-decision-process-dot-md><a href=/posts/kbhpolicy/#optimal-policy>optimal value function</a> for <a href=/posts/kbhpartially_observable_markov_decision_process/>POMDP</a></h2><p>\begin{equation}
U^{*}(b) = \max_{\pi} U^{\pi}(b)
\end{equation}</p><p>Of course, trying to actually do this is impossible because you have to iterate over all possible policies and then calculate every utility from them.</p><p>This is practically untenable, because the space of \(\pi\) is wayyy too big. Hence, we turn to <a href=/posts/kbhalpha_vector/>alpha vector</a>s.</p><p>See also <a href=/posts/kbhalpha_vector/#id-a2dee193-65b1-47ed-8dbc-aa362b28b451-optimal-value-function-for-pomdp-with-id-a11af4cf-7e36-4b3f-876f-e6a26cf6817e-alpha-vector>optimal value function for POMDP with alpha vector</a></p></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
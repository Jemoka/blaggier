<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>AML: It Takes Two</title>
<meta name=description content="Hello everyone! It&rsquo;s April, which means we are ready again for a new unit. Let&rsquo;s dive in.
You know what&rsquo;s better than one neural network? TWO!!! Multi-modal approaches&mdash;making two neural networks interact for a certain result&mdash;dominate many of the current edge of neural network research. In this unit, we are going to introduce one such approach, Generative Adversarial Networks (GAN), but leave you with some food for thought for other possibilities for what training multiple networks together can do."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>AML: It Takes Two</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p>Hello everyone! It&rsquo;s April, which means we are ready again for a new unit. Let&rsquo;s dive in.</p><p>You know what&rsquo;s better than one neural network? TWO!!! Multi-modal approaches&mdash;making two neural networks interact for a certain result&mdash;dominate many of the current edge of neural network research. In this unit, we are going to introduce one such approach, <strong>Generative Adversarial Networks</strong> (<strong>GAN</strong>), but leave you with some food for thought for other possibilities for what training multiple networks together can do.</p><p>Be aware that this unit will begin our more theory-focused discussions, and will leave more of the implementation up to your own explorations or a later fuller example. If you don&rsquo;t understand the math or the theory, please do flag us down in class or out to get things clarified.</p><h2 id=motivation>Motivation</h2><p>Although we will provide motivations for the architecture of a <strong>GAN</strong> in a bit, let&rsquo;s first provide a problem to ground ourselves.</p><p>Say we want to build a neural network to generate pictures of mountain goats. How would you do that?</p><p>You can&rsquo;t build a supervised model exactly: what&rsquo;s the input, and what are the labels? No clear answer. Even if you have labels, you&rsquo;d have infinitely many possible such mountain goats; how do you generate labels for all of those?</p><p>To help us in solving this problem, let us make a few related claims that may seem unmotivated for now:</p><ol><li>It is easy to find images of mountain goats <code>[citation needed]</code></li><li>It is eas(ier) to train a model to classify if an image is a mountain goat or not</li><li>It is easy to generate random noise</li><li>We want more pictures of mountain goats because they are cool</li></ol><p>It maybe unclear how <code>1-3</code> help us solve the mountain-goat generation problem; to explain why they are all crucial, we have to first understand some hand wavy game theory.</p><h2 id=better--motivation>(Better) Motivation</h2><p>It&rsquo;s storytime!</p><p>Al Capone and Eliot Ness are playing a game. Al is trying to create counterfeit Franklins, and Eliot is trying to catch them out of circulation.</p><p>Al first uses his HP Inkjet printer to print the currency. Eliot quickly wises up and uses a microscope to observe whether or not a piece of money in question is printed by ink or via color pigmented paper. Not wishing to foil his plan, Al asks his lab to develop new color pigmentation technology&mdash;just like the US currency does!</p><p>Yet, Eliot outsmarts Al again&mdash;he uses a spectrophotometer to analyze whether or not the money in question is made using paper or on cotton like the actual US currency. Seeing this, an angry Al purchases a tonne of cotton and starts printing his counterfeits on cotton.</p><p>Wanting to satisfy Jack&rsquo;s uselessly long analogy, Doctor Strange comes and freezes time for everyone except Al and Eliot (and their respective teams). As the true US currency technology remains the same, Eliot and Al continue to play this game: both developing better technologies to make or catch counterfeits.</p><p>After a billion years, Doctor Strange gets bored and looked into his frozen world. What does he see?</p><p>Al Capone built an exact replica of the US Mint.</p><p>Why? Each time Al gets caught out by Eliot, Al learns one more aspect of how his counterfeit differs from actual US currency. In effect, he&rsquo;s also learning one new detail of how the US currency is made. Therefore, if he keeps patching these tiny differences that Eliot helpfully pointed out for him for the span of a billion years, what Al will be producing will eventually be indistinguishable from US dollars as Eliot will be out of things to point out!</p><p>At this point, the Capone-Ness system has reached what we call <strong>Nash equilibrium</strong>: neither Eliot nor Al have a better move to make&mdash;Eliot no longer has anything more he can possibly do to catch counterfeits as Al&rsquo;s money is identical to US currency, and Al can no longer change his formula for money-making as any deviation will create another factor Eliot can latch onto.</p><h2 id=gans>GANs</h2><p>A <strong>Generative Adversarial Network</strong> (<strong>GAN</strong>) is a multi-modal generation model.</p><p>It is made out of two interacting neural networks:</p><ul><li><strong>generator</strong> \(G(x)\): Al Capone</li><li><strong>discriminator</strong> \(D(x)\): Eliot Ness</li></ul><p>Specifically, the <strong>generator</strong> is an unsupervised model trained on the task of generating the targets (&ldquo;images of mountain goats&rdquo;) from random noise, while the <strong>discriminator</strong> is a <strong>self-supervised model</strong> trained on the task of classifying whether or not something is actually the target (&ldquo;actual images of mountain goats&rdquo;) or the output of the generator.</p><p>The two models converge in tandem, in a similar fashion to the story discribed above.</p><h3 id=discriminator-d--x>Discriminator \(D(x)\)</h3><p>The <strong>discriminator</strong> \(D(x)\) is perhaps the more easily understandable model out of the two. It is a <strong>self-supervised model</strong> designed with the task of discriminating whether or not a particular input came from the actual world (&ldquo;goat images&rdquo;) or was the output of the <strong>generator</strong>.</p><p>Specifically, the <strong>discriminator</strong> is a neural network with any middle layers you&rsquo;d like that takes the output of the <strong>generator</strong> <em>or</em> real images as input, and produces a single <code>sigmoid</code> activated feature (between 0-1) where \(0\) represents &ldquo;definitely produced by <strong>generator</strong>&rdquo; and \(1\) represents &ldquo;definitely real world.&rdquo;</p><h3 id=generator-g--x>Generator \(G(x)\)</h3><p>The <strong>generator</strong> \(G(x)\) is a model that takes a <em>random tensor</em> as input and attempts to produce a generated sample (&ldquo;a picture of a goat&rdquo;). As with the discriminator, it can have any middle layers you&rsquo;d like but has to produce a tensor with the same shape and activation of an actual sample. For instance, if you are trying to produce images, the output of your <strong>generator</strong> has to be of shape \((channels, x, y)\) activated with <code>sigmoid</code> for brightness; if you are trying to produce single scalars, then the <strong>generator</strong> has to produce only value, etc.</p><p>It is perhaps very mystifying how we would ever build a magical box that takes a random tensor and turn it into a pretend image; looking at the loss functions (i.e. training objectives) of these two networks may perhaps help clarify this.</p><h3 id=loss-functions>Loss Functions</h3><p>Before we begin, I want to quickly reiterate something which will be crucial to your mental framework of the loss functions: <strong>THEY ARE NOT METRICS</strong>. The <em>value</em> of the loss functions&mdash;especially these ones&mdash;are now completely devoid of physical meaning; instead, the <em>trend</em> of the loss functions (&ldquo;value goes down means model is doing better&rdquo;) is what matters.</p><p>We are introducing the simplest form of <strong>GAN</strong> loss functions by <a href=https://arxiv.org/abs/1406.2661>Goodfellow, et al</a> called &ldquo;non-saturating loss.&rdquo; There are better ones, but these ones are mathematically elegant and works most of the time&mdash;and are the &ldquo;base case&rdquo; loss functions which other models improve on.</p><h4 id=discriminator-loss>Discriminator Loss</h4><p>\begin{equation}
L_{d} (\bold{x}_{i}, \bold{z}_{i}) = -\log D(\bold{x}_{i}) - \log (1- D(G(\bold{z}_{i})))
\end{equation}</p><p>where, \(\bold{x}_{i}\) is a tensor representing a real sample (for instance, again, an actual grid of pixels for a mountain goat image), and \(\bold{z}_{i}\) is a tensor containing random noise.</p><p>Woof. This is quite a scary loss function; let&rsquo;s break it up into pieces.</p><ul><li><p>\(-\log D(\bold{x}_{i})\): \(\bold{x}_{i}\) is a real sample, so we expect \(D\) to produce \(1\). Any value below \(1\) (i.e. the <strong>discriminator</strong> thinking a real image is generated) will produce negative values of increasingly larger magnitude as \(D(\bold{x}_{i})\) approaches \(0\). If the discriminator produces \(1\) correctly, \(\log 1 = 0\) and we indeed have converged.</p></li><li><p>\(-\log (1- D(G(\bold{z}_{i})))\): on the flip side, we expect the generator to consider the output of the generator (i.e. \(D(G(\bold{z}_{i}))\)) to be generated and produce \(0\). Therefore, we expect the same scheme as before but flipped (\(1-D(G(\bold{z}_{i})\))&mdash;if \(D(G(\bold{z}))\) produces \(1\) (&ldquo;the discriminator is fooled&rdquo;), \(1-D(G(\bold{z}))\) will produce \(0\) and the loss will be very high. Vise versa: if \(D(G(\bold{z}))\) produces \(0\) (&ldquo;the discriminator picked out the fake&rdquo;), the loss will be \(0\).</p><p>Adding the two values encourages our discriminator to both classify real samples as real \(1\), and generated samples as fake \(0\).</p></li></ul><h4 id=generator-loss>Generator Loss</h4><p>\begin{equation}
L_{g}(\bold{z}_{i}) = -\log (D(G(\bold{z}_{i})))
\end{equation}</p><p>The sharp-eyed among you may realize that this is just the right term from the above expression without the \(1-\) negation. Indeed, the training target for the <strong>generator</strong> is very simple: &ldquo;did I fool the discriminator&rdquo;: if \(D\) produces a large (close to \(1\)) output on the generated result&mdash;indicating that it is indeed &ldquo;fooled&rdquo;&mdash;our \(log\) will approach \(0\); whereas, if \(D\) produces a small (close to \(0\)) output on the generated result&mdash;indicating that it correctly spotted the fake&mdash;our \(log\) will produce a very negative value which creates high loss.</p><h2 id=the-gan-training-loop>The GAN Training Loop</h2><p>Loss functions in place, we are almost ready to make the model. The thing that&rsquo;s tricky about training a GAN is that we have to ensure that <em>both</em> the <strong>discriminator</strong> and <strong>generator</strong> are converging at the same exact time: ensuring that neither Capone nor Ness has <em>dramatically</em> better technology than the other. This requires a little bit of finesse on your part in terms of the training loop. Plus, our loss functions here are quite special, so their definitions will also need a little wrangling.</p><p>At this point, though, I hope we are all pretty confident in how to structure the basics of a ML model. Instead of going over that again, let&rsquo;s go over some of the differences in Python pseudo-code (code that doesn&rsquo;t run, but to illustrate how you would write it)&mdash;specially in four focus areas.</p><h3 id=dataprep>Dataprep</h3><p>Just a short note here on GAN data prep. What&rsquo;s the special thing about GANs? They are <strong>self-supervised</strong>&mdash;meaning they make their own labels. Instead, all you need to provide is plenty of examples of the thing you want your model to generate.</p><p>As such, your batch wouldn&rsquo;t contain <code>x_data</code>, <code>y_data</code>, etc. Instead, your dataset code should look something of the flavor:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>image_grid</span> <span style=color:#f92672>=</span> <span style=color:#111>example_data_for_the_gan_numpy</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>dataset</span> <span style=color:#f92672>=</span> <span style=color:#111>TensorDataset</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#111>image_grid</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>float</span><span style=color:#111>())</span> <span style=color:#75715e># only one argument!</span>
</span></span><span style=display:flex><span><span style=color:#111>dataloader</span> <span style=color:#f92672>=</span> <span style=color:#111>DataLoader</span><span style=color:#111>(</span><span style=color:#111>dataset</span><span style=color:#111>,</span> <span style=color:#111>batch_size</span><span style=color:#f92672>=</span><span style=color:#111>BATCH_SIZE</span><span style=color:#111>,</span> <span style=color:#111>shuffle</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>You will notice that the <code>TensorDataset</code> here took only <em>one</em> argument as input, as opposed to the usual 2: this is, as we discussed before, as product of the fact that our GAN only needs examples of the thing you want it to generate&mdash;no labels needed (or possible!)</p><h3 id=network-construction>Network Construction</h3><p>Of course, a GAN consists of two different networks. Though the network construction is mostly arbitrary, there are some general constraints:</p><h4 id=generator>generator</h4><ol><li><strong>input shape</strong>: arbitrary, but takes exclusively random values as input; ideally you want this to be the same number of dimensions as the output</li><li><strong>output shape</strong>: the <em>output shape</em> of your network has to be the shape of one sample of the real data as the generator should generate something that looks like real data</li><li><strong>output activation</strong>: whatever makes sense for the real data: if probabilities, then <code>softmax</code>; if images, then <code>sigmoid</code> (as normalized brightness), etc.</li></ol><h4 id=discriminator>discriminator</h4><ol><li><strong>input shape</strong>: the <em>output shape</em> of the generator, or the shape of one real sample of data. (<em>Thinking Break</em>: WHY? as usual, pause and chat)</li><li><strong>output shape</strong>: <code>(batch_size, 1)</code>. We want to output a scalar between \(0\) (&ldquo;probably fake&rdquo;) and \(1\) (&ldquo;probably real&rdquo;) for every sample</li><li><strong>output activation</strong>: <code>sigmoid</code> to get those values actually between \(0\) and \(1\)</li></ol><h3 id=network-initialization>Network Initialization</h3><p>Because the generator and discriminator are two different networks, they require different optimizers!</p><p>So, we have to go about making them. This is fortunately pretty direct:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># initialize networks</span>
</span></span><span style=display:flex><span><span style=color:#111>gen</span> <span style=color:#f92672>=</span> <span style=color:#111>GeneratorNetwork</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>disc</span> <span style=color:#f92672>=</span> <span style=color:#111>DiscriminatorNetwork</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># initalize *two seperate optimizers*</span>
</span></span><span style=display:flex><span><span style=color:#111>gen_optim</span> <span style=color:#f92672>=</span> <span style=color:#111>Adam</span><span style=color:#111>(</span><span style=color:#111>gen</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#111>LR1</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>disc_optim</span> <span style=color:#f92672>=</span> <span style=color:#111>Adam</span><span style=color:#111>(</span><span style=color:#111>disc</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#111>LR2</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>Nothing out of the ordinary here, but a worthy reminder that you need 2. This will become important shortly.</p><h3 id=training-loop>Training Loop</h3><p>This is the main event, and probably the bit that most people trip up the most: the training loop. Let&rsquo;s see a pseudocode implementation of one, and we will discuss how its structured.</p><p><em>Note that we will be making some adjustments to our tried-and-true backprop logic.</em></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#111>EPOCHS</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>for</span> <span style=color:#111>batch</span> <span style=color:#f92672>in</span> <span style=color:#111>iter</span><span style=color:#111>(</span><span style=color:#111>dataloader</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># train generator first</span>
</span></span><span style=display:flex><span>        <span style=color:#111>disc_score</span> <span style=color:#f92672>=</span> <span style=color:#111>disc</span><span style=color:#111>(</span><span style=color:#111>gen</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#111>BATCH_SIZE</span><span style=color:#111>,</span><span style=color:#111>YOUR</span><span style=color:#111>,</span><span style=color:#111>INPUT</span><span style=color:#111>,</span><span style=color:#111>SHAPE</span><span style=color:#111>,</span><span style=color:#111>HERE</span><span style=color:#111>)))</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># compute + backprop generator loss</span>
</span></span><span style=display:flex><span>        <span style=color:#111>generator_loss</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#111>disc_score</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>generator_loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># disconnect discriminator gradients</span>
</span></span><span style=display:flex><span>        <span style=color:#111>disc_optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># step and clear</span>
</span></span><span style=display:flex><span>        <span style=color:#111>gen_optim</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>gen_optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># now, train discriminator</span>
</span></span><span style=display:flex><span>        <span style=color:#111>disc_score_false</span> <span style=color:#f92672>=</span> <span style=color:#111>disc</span><span style=color:#111>(</span><span style=color:#111>gen</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#111>BATCH_SIZE</span><span style=color:#111>,</span><span style=color:#111>YOUR</span><span style=color:#111>,</span><span style=color:#111>INPUT</span><span style=color:#111>,</span><span style=color:#111>SHAPE</span><span style=color:#111>,</span><span style=color:#111>HERE</span><span style=color:#111>))</span><span style=color:#f92672>.</span><span style=color:#111>detach</span><span style=color:#111>())</span>
</span></span><span style=display:flex><span>        <span style=color:#111>disc_score_true</span> <span style=color:#f92672>=</span> <span style=color:#111>disc</span><span style=color:#111>(</span><span style=color:#111>batch</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># compute + backprop discriminator loss</span>
</span></span><span style=display:flex><span>        <span style=color:#111>discriminator_loss</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#111>disc_score_true</span><span style=color:#111>)</span><span style=color:#f92672>-</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>-</span><span style=color:#111>disc_score_false</span><span style=color:#111>))</span>
</span></span><span style=display:flex><span>        <span style=color:#111>discriminator_loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># step and clear</span>
</span></span><span style=display:flex><span>        <span style=color:#111>disc_optim</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>        <span style=color:#111>disc_optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>Woweee. Much to talk about. Let&rsquo;s break it down.</p><h4 id=scoring-on-fake-sample>Scoring on fake sample</h4><p>We first generate a fake sample from the generator by first passing it random noise from <code>torch.rand</code>, then passing its output to the discriminator to get a group of scores.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>disc_score</span> <span style=color:#f92672>=</span> <span style=color:#111>disc</span><span style=color:#111>(</span><span style=color:#111>gen</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#111>BATCH_SIZE</span><span style=color:#111>,</span><span style=color:#111>YOUR</span><span style=color:#111>,</span><span style=color:#111>INPUT</span><span style=color:#111>,</span><span style=color:#111>SHAPE</span><span style=color:#111>,</span><span style=color:#111>HERE</span><span style=color:#111>)))</span>
</span></span></code></pre></div><h4 id=calculating-the-generator-loss>Calculating the generator loss</h4><p>Next up, we will calculate the generator loss on the score that the discriminator gave for that fake sample we generated earlier.</p><p>Recall that:</p><p>\begin{equation}
L_{g}(\bold{z}_{i}) = -\log (D(G(\bold{z}_{i})))
\end{equation}</p><p>and hence:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>generator_loss</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#111>disc_score</span><span style=color:#111>))</span>
</span></span></code></pre></div><p><span class=underline>Thinking break!</span>: why does implementing <code>(-torch.log(disc_score))</code> accomplish the same thing as taking \(-\log (D(G(\bold{z}_{i})))\)? Specifically, how is <code>disc_score</code> calculated in our example?</p><h4 id=the-generator-backprop-step>The generator backprop step</h4><p>For all that drilling we did of BACKPROP! STEP! RESET!, the next step may feel sacrilegious:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>generator_loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># disconnect discriminator gradients</span>
</span></span><span style=display:flex><span><span style=color:#111>disc_optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># step and clear</span>
</span></span><span style=display:flex><span><span style=color:#111>gen_optim</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>gen_optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span></code></pre></div><p><em>What is happening here?</em> Let&rsquo;s take it one step at a time.</p><p>First, we call <code>generator_loss.backward()</code> to backprop the loss; nothing wrong here. But then, against all odds, we call <code>.zero_grad()</code> on the <strong>discriminator</strong> optimizer. What gives?</p><p>Recall that, in this case, we are training the <strong>generator</strong>; as the loss-function literally asks the <strong>discriminator</strong> to be wrong, we mustn&rsquo;t be updating the discriminator using the gradients computed against this function; instead, we simply want the generator to be updated to better fool the <strong>discriminator</strong>.</p><p>Therefore, we immediately zero out all the gradients on the <strong>discriminator</strong> to prevent this step from updating the <strong>discriminator</strong> with the &ldquo;fooling&rdquo; loss function; and proceed to update the <strong>generator</strong> weights as usual.</p><h4 id=scoring-on-detached-fake-sample-and-real-sample>Scoring on detached fake sample and real sample</h4><p>Next up, training the <strong>discriminator</strong>. We first obtain scores from the discriminator for a real sample and a fake sample separately:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>disc_score_false</span> <span style=color:#f92672>=</span> <span style=color:#111>disc</span><span style=color:#111>(</span><span style=color:#111>gen</span><span style=color:#111>(</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>rand</span><span style=color:#111>(</span><span style=color:#111>BATCH_SIZE</span><span style=color:#111>,</span><span style=color:#111>YOUR</span><span style=color:#111>,</span><span style=color:#111>INPUT</span><span style=color:#111>,</span><span style=color:#111>SHAPE</span><span style=color:#111>,</span><span style=color:#111>HERE</span><span style=color:#111>))</span><span style=color:#f92672>.</span><span style=color:#111>detach</span><span style=color:#111>())</span>
</span></span><span style=display:flex><span><span style=color:#111>disc_score_true</span> <span style=color:#f92672>=</span> <span style=color:#111>disc</span><span style=color:#111>(</span><span style=color:#111>batch</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>You should notice that the code here for obtaining the fake sample is almost identical to the one before; except, we are calling this <code>.detach()</code> against the generator output. This is very functionally similar to the &ldquo;calling <code>.zero_grad()</code> immediately&rdquo; move we made earlier; called <code>.detach()</code> asks PyTorch to treat whatever tensor there as a constant, and not propagate gradients any more backwards into the <strong>generator</strong>, which in this case we do not want to change as we are optimizing the <strong>discriminator</strong>.</p><h4 id=calculating-the-discriminator-loss>Calculating the discriminator loss</h4><p>With all the pieces in place, this is again just a very directly implementation of:</p><p>\begin{equation}
L_{d} (\bold{x}_{i}, \bold{z}_{i}) = -\log D(\bold{x}_{i}) - \log (1- D(G(\bold{z}_{i})))
\end{equation}</p><p>in code.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>discriminator_loss</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#f92672>-</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#111>disc_score_true</span><span style=color:#111>)</span><span style=color:#f92672>-</span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>log</span><span style=color:#111>(</span><span style=color:#ae81ff>1</span><span style=color:#f92672>-</span><span style=color:#111>disc_score_false</span><span style=color:#111>))</span>
</span></span></code></pre></div><h4 id=normal-backprop>Normal backprop</h4><p>Because we ran <code>.detach()</code> before on the <strong>generator</strong> output, the <strong>generator</strong> is treated as a constant through this second loss function; as such, our backpropegation step will normally update the <strong>discriminator</strong>&rsquo;s weights without any fuss. We therefore go back to our tried-and-true formula:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>discriminator_loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>disc_optim</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>disc_optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>Tada! That&rsquo;s it; the GAN training loop.</p><h2 id=final-thoughts-and-unit-challenge>Final Thoughts and Unit Challenge</h2><p>Sorry for the very theoretically dense unit; please don&rsquo;t hesitate to flag us down if any questions take place. To leave you, here are a few final tips and tricks for making GANs.</p><ol><li>If your model doesn&rsquo;t work, try <strong>pretraining</strong> the <strong>discriminator</strong>: letting Eliot Ness get a bit of a head start by training the discriminator to recognize noise from real images; to do this, just don&rsquo;t run the code that updates the generator weights.</li><li>GANs are known to perform something called <strong>mode collapse</strong>: whereby, instead of reaching <strong>Nash equilibrium</strong>, one of the two networks crash while the other one completely converges. One attempt to solve this is something called <strong>Wassterstein Loss</strong>, which is <a href=https://developers.google.com/machine-learning/gan/loss#wasserstein-loss>discussed here</a> (<a href=https://developers.google.com/machine-learning/gan/loss#wasserstein-loss>https://developers.google.com/machine-learning/gan/loss#wasserstein-loss</a>). One important note, however, is that using this loss function makes your network <em>technically</em> not a GAN anymore (as the <strong>discriminator</strong> will not be actually usefully discriminating, instead acting as a &ldquo;<strong>critic</strong>&rdquo; for the generator only producing non-interpretable scores), but it has shown improved performance for the <strong>generator</strong> only.</li><li>GANs are notoriously hard to make work. <a href=https://developers.google.com/machine-learning/gan/problems>See this whole page from Google</a> (<a href=https://developers.google.com/machine-learning/gan/loss>https://developers.google.com/machine-learning/gan/loss</a>) about the various ways GANs can fail and possible strategies to remedy them. <strong>Do not</strong> be scared if your model doesn&rsquo;t work immediately or even after copious tuning.</li></ol><p>Ok, onto the challenge: make a GAN! There are two variants of this:</p><ol><li>Easier &mdash; use a pair of <strong>dense neural networks</strong> to make a GAN to generate valid series of \(5\) numbers which we explored in the beginning of this class \([a,b,c,c+1,c+2]\)</li><li>Harder &mdash; use a pair of <strong>convolutional neural networks</strong> to make a GAN to generate <a href=https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz>these nice pictures of pets</a> (<a href=https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz>https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz</a>). Sorry that this is not mountain goats: unfortunately, a dataset large enough is not available for this task :/</li></ol><p>Good luck, and have fun!</p></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
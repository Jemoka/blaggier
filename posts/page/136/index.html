<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title>
<meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Posts</div></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><main><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhiclr2025_snell_scaling_llm_test_time_compute/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhiclr2025_snell_scaling_llm_test_time_compute/>ICLR2025 Snell: Optimality of Scaling LLM Test-Time Compute</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=compute-optimal-scaling>Compute-Optimal Scaling</h2><p><a href=#compute-optimal-scaling>Compute-Optimal Scaling</a> is the notion of selecting the optimal configuration (beam width, search budget, etc.) dynamically / for binned question.</p><h2 id=approaches-to-scaling-test-time-compute>Approaches to &ldquo;Scaling Test-Time Compute&rdquo;</h2><p>Three primary approaches:</p><ul><li>best-of-n: roll out a bunch, reject</li><li><a href=/posts/kbhnlp/#beam-search>Beam Search</a>: check against intermediate</li><li>lookahead search: MCTSish (do lookahead rollouts)</li></ul><h3 id=key-insight>Key insight</h3><ul><li>On easy qusetion, beam search shows over-optimization and best of n is good</li><li>on medium/hard questions, beam search is better</li></ul><p>Lookahead seems bad?</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhiclr2025_thursday_morning_posters/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhiclr2025_thursday_morning_posters/>ICLR2025 Thursday Morning Posters</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=iclr2025-hu-belief-state-transformer>ICLR2025 Hu: belief state transformer</h2><p>Key insight: residual stream at the last token kept thought of as a belief state encoding future tokens, that is, uncertainty in the last residual directly correlate the diversity of output</p><p>Method: trainer transformer and trainer reverse transformer like what Robert wanted, then correlate</p><h2 id=iclr2025-lingam-diversity-of-thoughts>ICLR2025 Lingam: diversity of thoughts</h2><p>Key insight: Use iterative sampling to achieve higher diversity in self reflection, in order to get better outputs.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhiclr2025_tokenizer_free_approaches/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhiclr2025_tokenizer_free_approaches/>ICLR2025 Tokenizer-Free Approaches</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=talks>Talks</h2><ul><li><a href=/posts/kbhiclr2025_kilani_mrt5_tokenizer_free/>ICLR2025 Kilani: MrT5 Tokenizer-Free</a></li><li><a href=/posts/kbhiclr2025_neitemeier_hierachical_autoregressive_transformers/>ICLR2025 Neitemeier: Hierachical Autoregressive Transformers</a></li></ul><h2 id=downsides-of-subword-tokenization--kbhtokenization-dot-md>Downsides of <a href=/posts/kbhtokenization/#subword-tokenization>Subword Tokenization</a></h2><ol><li><strong>not learned end to end</strong>: vocab is fixed, can&rsquo;t adapt to difficulty</li><li><strong>non-smoothness</strong>: similar inputs get mapped to very different token sequences<ul><li>[token][ization]</li><li>typo: [token][zi][ation] &lt;- suddenly bad despite small typo</li></ul></li><li><strong>huge vocabs</strong>: yes</li><li><strong>non-adaptive compression ratio</strong>: you can&rsquo;t decide how much to compress (affects FLOPs/document)</li></ol></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhiclr2025_wu_retrieval_head_explains_long_context/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhiclr2025_wu_retrieval_head_explains_long_context/>ICLR2025 Wu: Retrieval Head Explains Long Context</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=motivation>Motivation</h2><p>Previous works contain &ldquo;heads&rdquo; that perform some specific mechanism from context retrieval.</p><h2 id=retrieval-head>Retrieval Head</h2><p>Authors shows that <a href=#retrieval-head>Retrieval Head</a>s exist in transformers: using Needle in a Haystack framework.</p><h2 id=key-insight>Key Insight</h2><p>There exists certain heads which performs retrieval, as measured by the <a href=#measuring-retrieval-behavior>retrieval score</a>.</p><h2 id=methods>Methods</h2><h3 id=measuring-retrieval-behavior>Measuring Retrieval Behavior</h3><p>&ldquo;retrieval score&rdquo;: how often does a head engage in copy-paste behavior.</p><ol><li><strong>token inclusion</strong>: current generated token \(w\) is in the edle</li><li><strong>maximal attention</strong>: same token gives the maximum attenion score</li></ol></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhiclr2025_yue_inference_scaling_for_rag/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhiclr2025_yue_inference_scaling_for_rag/>ICLR2025 Yue: Inference Scaling for Long-Context RAG</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>&ldquo;RAG performance can scale almost linearly w.r.t. log inference FLOPs&rdquo;</p><h2 id=demonstration-based-rag--drag>Demonstration Based RAG (DRAG)</h2><h3 id=method>Method</h3><p>Adding demonstrations as k in-context examples.</p><p>Prompt: documents, input query, final answer.</p><p>Parameters: number of documents, number of in context samples, number of iterations upper bound.</p><h2 id=iterative-demonstration-based-rag--iterdrag>Iterative Demonstration Based RAG (IterDRAG)</h2><h3 id=method>Method</h3><p>DRAG above, and then the model can generate a new sub-query. The model decides</p><p>Parameters: number of documents, number of in context samples, number of iterations upper bound.</p></span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/135/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/134/ aria-label="Page 134" class=page-link role=button>134</a></li><li class=page-item><a href=/posts/page/135/ aria-label="Page 135" class=page-link role=button>135</a></li><li class="page-item active"><a aria-current=page aria-label="Page 136" class=page-link role=button>136</a></li><li class=page-item><a href=/posts/page/137/ aria-label="Page 137" class=page-link role=button>137</a></li><li class=page-item><a href=/posts/page/138/ aria-label="Page 138" class=page-link role=button>138</a></li><li class=page-item><a href=/posts/page/137/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/367/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
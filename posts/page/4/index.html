<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title>
<meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Posts</div></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><main><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhrelativization_barrier_to_p_vs_np/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhrelativization_barrier_to_p_vs_np/>(NOTES COPY) Relativization Barrier to P vs. NP</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>Key Takeaway: a powerful technique, diagonalization, is doomed to fail at resolving P vs. NP.</p><h2 id=context>Context</h2><p>Complexity theory is generally the study of impossibilities. This is also an impossibility result. Here are some impossibility results!</p><ol><li>The reals are uncountable [Cantor 1874]</li><li><a href=/posts/kbhmathematics/#godel-s-incompleteness>Godel&rsquo;s incompleteness</a> theorem [Godel 1931]</li><li><a href=/posts/kbhmapping_reduction/#halting-problem>halting problem</a> is <a href=/posts/kbhturing_machinea/#decidable>undecidable</a> [Turing 1936]</li><li><a href=/posts/kbhtime_complexity/#time-hierarchy-theorem>time hierarchy theorem</a> [Hartmanis-Stearns 1965] &mdash; \(\text{P}\) is a strict subset of \(\text{EXP}\)</li></ol><p>Notice! All of these theorems uses a <strong>single technique</strong>&mdash;diagonalization.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhllms_are_suprisingly_useless/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhllms_are_suprisingly_useless/>(Realistic) LLMs are not super powerful</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=introduction>Introduction</h2><p>Large language models (LLMs) have transformed much of the field of natural language processing (NLP) (<a href=#citeproc_bib_item_10>Radford et al., n.d.</a>; <a href=#citeproc_bib_item_12>Vaswani et al. 2017</a>; <a href=#citeproc_bib_item_3>Devlin et al. 2019</a>; <a href=#citeproc_bib_item_11>Raffel et al. 2023</a>), the study of human languages. Its success, in some sense, is a little surprising: humans essentially have made for themselves a general &ldquo;reasoning&rdquo; algorithm (albeit a pretty bad one as of right now) using entirely inexact Bayesian modeling approaches. Exactly since LLMs developed from literature in the Bayesian modeling community, &ldquo;formal&rdquo; questions about what an LLM can or cannot do&mdash;the complexity-theoretic elements of LLMs as models&mdash;are comparatively understudied. Even now, we have very little idea of what an LLM is even learning, let alone trying to understand how to steer them or bound their behavior (<a href=#citeproc_bib_item_13>Wu et al. 2025</a>)&mdash;which, for a purported thinking machine, is a thing we probably want.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhbit_signal_processing/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhbit_signal_processing/>(signal processing)</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>Bits is a universal currency to transmit information. As long as we can encode</p><h2 id=source-channel-separation-theorem>Source-Channel Separation Theorem</h2><p>&ldquo;Are we loosing information by using Bits? No.&rdquo;</p><p>Statement:</p><p>If a source can be transmitted over a channel at a certain resolution, then it can be transmitted using a binary interface between the source and channel at the same resolution.</p><p>Meaning:</p><p>If a communication channel has a certain fidelity for arbitrary data, encoding the data by bits will not change the fidelity of the channel.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhzero_times_vector/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhzero_times_vector/>0v=0</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>\begin{align}
0v &= (0+0)v \\
&= 0v+0v
\end{align}</p><p>Given <a href=/posts/kbhscalar_multiplication/>scalar multiplication</a> is <a href=/posts/kbhclosed/>closed</a>, \(0v \in V\), which means \(\exists -0v:0v+(-0v)=0\). Applying that to both sides:</p><p>\begin{equation}
0 = 0v\ \blacksquare
\end{equation}</p><p>The opposite proof of \(\lambda 0=0\) but vectors work the same exact way.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbheigenvalue/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbheigenvalue/>1-d invariant subspace</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p><a href=/posts/kbheigenvalue/>eigenvalue</a> is the scalar needed to scale the <a href=/posts/kbhbasis/>basis</a> element of a one <a href=/posts/kbhdimension/>dimension</a>al <a href=/posts/kbhinvariant_subspace/>invariant subspace</a> of a <a href=/posts/kbhlinear_map/>Linear Map</a> to represent the behavior of the map:</p><p>\begin{equation}
Tv = \lambda v
\end{equation}</p><p>Note we require \(v \neq 0\) because otherwise all scalars count.</p><p><a href=/posts/kbheigenvalue/>eigenvector</a> is a <a href=/posts/kbhvector/>vector</a> that forms the <a href=/posts/kbhbasis/>basis</a> list of length 1 of that 1-D <a href=/posts/kbhinvariant_subspace/>invariant subspace</a> under \(T\).</p><p>&ldquo;<a href=/posts/kbhoperator/>operator</a>s own <a href=/posts/kbheigenvalue/>eigenvalue</a>s, <a href=/posts/kbheigenvalue/>eigenvalue</a>s own <a href=/posts/kbheigenvalue/>eigenvector</a>s&rdquo;</p><p>Why is <a href=/posts/kbheigenvalue/>eigenvalue</a> consistent per <a href=/posts/kbheigenvalue/>eigenvector</a>? Because a linear map has to act on the same way to something&rsquo;s basis as it does to the whole space.</p></span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/3/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/2/ aria-label="Page 2" class=page-link role=button>2</a></li><li class=page-item><a href=/posts/page/3/ aria-label="Page 3" class=page-link role=button>3</a></li><li class="page-item active"><a aria-current=page aria-label="Page 4" class=page-link role=button>4</a></li><li class=page-item><a href=/posts/page/5/ aria-label="Page 5" class=page-link role=button>5</a></li><li class=page-item><a href=/posts/page/6/ aria-label="Page 6" class=page-link role=button>6</a></li><li class=page-item><a href=/posts/page/5/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/367/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title>
<meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Posts</div></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><main><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhlanguage/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhlanguage/>language</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p><a href=/posts/kbheffability/>effability</a></p><p>see also <a href=/posts/kbhalphabet/>language</a></p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhlanguage_agents/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhlanguage_agents/>Language Agents with Karthik</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=transitions>Transitions</h2><ol><li>Transition first from rule based learning to statistical learning</li><li>Rise of semantic parsing: statistical models of parsing</li><li>Then, moving from semantic parsing to large models&mdash;putting decision making and language modeling into the same bubble</li></ol><h2 id=importance-of-llms>Importance of LLMs</h2><ul><li>They are simply better at understanding language inputs</li><li>They can generate structured information (i.e. not just human language, JSONs, etc.)</li><li>They can perform natural language &ldquo;reasoning&rdquo;&mdash;not just generate</li></ul><p>(and natural language generation, abv)</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhlanguage_information_index/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhlanguage_information_index/>Language Information Index</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>What makes language modeling hard: <strong>resolving ambiguity is hard</strong>.</p><p>&ldquo;the chef made her duck&rdquo;</p><h2 id=contents>Contents</h2><h3 id=basic-text-processing>Basic Text Processing</h3><ul><li><a href=/posts/kbhregex/>regex</a></li><li><a href=/posts/kbheliza/>ELIZA</a></li><li><a href=/posts/kbhtokenization/>tokenization</a> and <a href=/posts/kbhcorpus/>corpus</a><ul><li><a href=/posts/kbhcorpus/#herdan-s-law>Herdan&rsquo;s Law</a></li></ul></li><li><a href=/posts/kbhtext_normalization/>text normalization</a><ul><li><a href=/posts/kbhtokenization/>tokenization</a> + <a href=/posts/kbhtokenization/#subword-tokenization>Subword Tokenization</a><ul><li><a href=/posts/kbhbpe/>BPE</a></li></ul></li><li><a href=/posts/kbhword_normalization/>Word Normalization</a><ul><li><a href=/posts/kbhlemmatization/>lemmatization</a> through <a href=/posts/kbhmorphological_parsing/>morphological parsing</a></li><li>only take stems from morphemes: <a href=/posts/kbhmorphological_parsing/#porter-stemmer>porter stemmer</a></li></ul></li><li><a href=/posts/kbhsentence_segmentation/>sentence segmentation</a></li></ul></li><li><a href=/posts/kbhn_grams/>N-Grams</a></li></ul><h3 id=edit-distance>Edit Distance</h3><p>DP costs \(O(nm)\), backtrace costs \(O(n+m)\).</p><ul><li><a href=/posts/kbhminimum_edit_distance/>minimum edit distance</a><ul><li><a href=/posts/kbhweighted_edit_distance/>weighted edit distance</a></li></ul></li><li><a href=/posts/kbhbacktracing/>backtracing</a></li></ul><h3 id=ngrams>Ngrams</h3><ul><li><a href=/posts/kbhn_grams/>N-Grams</a></li><li><a href=/posts/kbhn_grams/#markov-assumption>Markov Assumption</a><ul><li><a href=/posts/kbhn_grams/#unigrams>Unigrams</a></li><li><a href=/posts/kbhn_grams/#backoff>Backoff</a> and <a href=/posts/kbhn_grams/#stupid-backoff>Stupid Backoff</a></li><li><a href=/posts/kbhn_grams/#interpolation>Interpolation</a></li><li><a href=/posts/kbhn_grams/#oov-words>OOV Words</a></li></ul></li><li><a href=/posts/kbhmodel_evaluation/>Model Evaluation</a><ul><li><a href=/posts/kbhperplexity/>perplexity</a></li><li><a href=/posts/kbhn_grams/#oov-words>open vocabulary</a></li></ul></li></ul><h3 id=text-classification>Text Classification</h3><ul><li><a href=/posts/kbhtext_classification/>Text Classification</a><ul><li><a href=/posts/kbhbag_of_words/>Bag of Words</a></li><li><a href=/posts/kbhnaive_bayes/>Naive Bayes</a></li><li><a href=/posts/kbhbag_of_words/#naive-bayes-for-text-classification>Naive Bayes for Text Classification</a><ul><li><a href=/posts/kbhbag_of_words/#binary-naive-bayes>Binary Naive Bayes</a></li></ul></li><li><a href=/posts/kbhlexicon/>Lexicon</a></li></ul></li><li><a href=/posts/kbhn_grams/#naive-bays-language-modeling>Naive Bays Language Modeling</a></li><li><a href=/posts/kbhharmonic_mean/>Harmonic Mean</a><ul><li><a href=/posts/kbhmacroaverage/>Macroaverage</a> and <a href=/posts/kbhmacroaverage/>Microaverage</a></li></ul></li></ul><h3 id=logistic-regression>Logistic Regression</h3><ul><li><a href=/posts/kbhgenerative_vs_discriminitive_classifier/#generative-classifier>Generative Classifier</a> vs <a href=/posts/kbhgenerative_vs_discriminitive_classifier/#discriminative-classifier>Discriminate Classifier</a></li><li><a href=/posts/kbhlogistic_regression/#logistic-regression-text-classification>Logistic Regression Text Classification</a><ul><li><a href=/posts/kbhlogistic_regression/#logistic-regression-text-classification>decision boundary</a></li></ul></li><li><a href=/posts/kbhcross_entropy_loss/>cross entropy loss</a></li><li><a href=/posts/kbhstochastic_gradient_descent/>stochastic gradient descent</a></li></ul><h3 id=information-retrial>Information Retrial</h3><ul><li><a href=/posts/kbhinformation_retrival/>Information Retrival</a><ul><li><a href=/posts/kbhterm_document_matrix/>Term-Document Matrix</a></li><li><a href=/posts/kbhinverted_index/>Inverted Index</a> + <a href=/posts/kbhinverted_index/#postings-list>postings list</a><ul><li><a href=/posts/kbhinverted_index/#boolean-retrieval>Boolean Retrieval</a></li><li><a href=/posts/kbhinverted_index/#positional-index>positional index</a></li></ul></li></ul></li></ul><h3 id=ranked-information-retrial>Ranked Information Retrial</h3><ul><li><a href=/posts/kbhranked_information_retrieval/>Ranked Information Retrieval</a><ul><li><a href=/posts/kbhranked_information_retrieval/#feast-or-famine-problem>feast or famine problem</a></li><li><a href=/posts/kbhranked_information_retrieval/#free-text-query>free text query</a></li></ul></li><li><a href=/posts/kbhranked_information_retrieval/#score>score</a><ul><li><a href=/posts/kbhranked_information_retrieval/#jaccard-coefficient>Jaccard Coefficient</a></li><li><a href=/posts/kbhranked_information_retrieval/#log-frequency-weighting>log-frequency weighting</a></li><li><a href=/posts/kbhranked_information_retrieval/#document-frequency>document frequency</a> ("<a href=/posts/kbhranked_information_retrieval/#document-frequency>idf weight</a>")</li><li><a href=/posts/kbhranked_information_retrieval/#tf-idf>TF-IDF</a><ul><li><a href=/posts/kbhranked_information_retrieval/#smart-notation>SMART notation</a></li></ul></li></ul></li><li><a href=/posts/kbhranked_information_retrieval/#vector-space-model>vector-space model</a></li></ul><h3 id=vector-semantics>Vector Semantics</h3><ul><li><a href=/posts/kbhsense/>sense</a><ul><li><a href=/posts/kbhsense/#principle-of-contrast>principle of contrast</a></li><li><a href=/posts/kbhsense/#word-relatedness>word relatedness</a><ul><li><a href=/posts/kbhsense/#semantic-field>semantic field</a></li></ul></li><li><a href=/posts/kbhsense/#synonymy>synonymy</a> and <a href=/posts/kbhsense/#antonyms>antonyms</a></li><li><a href=/posts/kbhsense/#affective-meaning>affective meaning</a></li></ul></li><li><a href=/posts/kbhvector_semantics/>vector semantics</a><ul><li><a href=/posts/kbhvector_semantics/#transposing-a-id-b5d7f908-0351-436d-9784-180ab5aa0562-term-document-matrix>transposing a Term-Document Matrix</a></li><li><a href=/posts/kbhvector_semantics/#term-term-matrix>term-term matrix</a></li><li><a href=/posts/kbhword2vec/>word2vec</a><ul><li><a href=/posts/kbhword2vec/#skip-gram-with-negative-sampling>skip-gram with negative sampling</a></li></ul></li></ul></li></ul><h3 id=pos-and-ner>POS and NER</h3><ul><li><a href=/posts/kbhpos_tagging/>POS Tagging</a></li><li><a href=/posts/kbhner_tagging/>NER Tagging</a></li></ul><h3 id=dialogue-systems>Dialogue Systems</h3><ul><li><a href=/posts/kbhdialogue/>Dialogue</a></li><li><a href=/posts/kbhchatbot/>Chatbot</a><ul><li><a href=/posts/kbhparry/>PARRY</a></li></ul></li></ul><h3 id=recommender-systems>Recommender Systems</h3><ul><li><a href=/posts/kbhrecommender_system/>Recommender System</a></li></ul><h3 id=dora>Dora</h3><ul><li><a href=/posts/kbhdora/>Dora</a></li></ul><h3 id=neural-nets>Neural Nets</h3><ul><li><a href=/posts/kbhneural_networks/>Neural Networks</a></li></ul><h3 id=the-web>The Web</h3><ul><li><a href=/posts/kbhweb_graph/>Web Graph</a></li><li><a href=/posts/kbhsocial_network/>Social Network</a></li></ul></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhlanguage_model/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhlanguage_model/>Language Model</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>A <a href=/posts/kbhmachine_learning/>machine learning</a> model: <strong>input</strong> &mdash; last n words, <strong>output</strong> &mdash; probabilist distribution over the next word. An LM predicts this distribution (&ldquo;what&rsquo;s the distribution of next word given the previous words):</p><p>\begin{equation}
W_{n} \sim P(\cdot | w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)})
\end{equation}</p><p>By applying the chain rule, we can also think of the language model as assigning a probability to a sequence of words:</p><p>\begin{align}
P(S) &= P(w^{(t)} | w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)}) \cdot P(w^{(t-1)} | w^{(t-2)}, \dots, w^{(1)}) \dots \\
&= P(w^{(t)}, w^{(t-1)}, w^{(t-2)}, \dots, w^{(1)})
\end{align}</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhinteractive_agent/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhinteractive_agent/>Language Model Agents</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>agents that uses the language to act on behave of another person or group.</p><h2 id=challenges>Challenges</h2><p>See <a href=/posts/kbhchallenges_of_language_model_agents/>Challenges of Language Model Agents</a></p><h2 id=methods>Methods</h2><h3 id=react>ReAct</h3><p>See <a href>ReAct</a></p><h3 id=aguvis>Aguvis</h3><p>Take the <a href=/posts/kbhagentnet/>AgentNet</a> dataset, and then tune a vison LM to roll out the rest of the sequence of actions given screenshots as input on top of a Qwen base model.</p><p>We can also add on top <a href=/posts/kbhchain_of_thought/>Chain of Thought</a> to get more thinking as well.</p><h2 id=formulations>Formulations</h2><h3 id=osworld>OSWorld</h3><p>A unified task setup and evaluation.</p></span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/150/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/149/ aria-label="Page 149" class=page-link role=button>149</a></li><li class=page-item><a href=/posts/page/150/ aria-label="Page 150" class=page-link role=button>150</a></li><li class="page-item active"><a aria-current=page aria-label="Page 151" class=page-link role=button>151</a></li><li class=page-item><a href=/posts/page/152/ aria-label="Page 152" class=page-link role=button>152</a></li><li class=page-item><a href=/posts/page/153/ aria-label="Page 153" class=page-link role=button>153</a></li><li class=page-item><a href=/posts/page/152/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/367/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
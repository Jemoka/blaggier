<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title>
<meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Posts</div></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><main><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_cs224n_apr112024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_cs224n_apr112024/>SU-CS224N APR112024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=linguistic-structure>Linguistic Structure</h2><p>Humans somehow turn linear into complex meaning with bigger, non-linear units. We need to make explicit this structural complexity. Sometimes, this is even ambiguous.</p><p>We can use this to extract information from human languages.</p><h3 id=why-is-this-hard>Why is this hard?</h3><ul><li>coding: <strong>global clarity, local ambiguity</strong> (number of white spaces doesn&rsquo;t matter, but code always have one exact meaning)</li><li>speaking: <strong>global ambiguity, local clarity</strong> (words are always clearly said, but what they refer to maybe unclear)</li></ul><h4 id=prepositional-ambiguity>Prepositional Ambiguity</h4><p>Why? &mdash; Prepositional Phrase does not have clear attachment. The sequence of possible attachments grows exponentially.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_cs224n_apr162024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_cs224n_apr162024/>SU-CS224N APR162024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=why-do-neural-nets-work-suddenly>Why do Neural Nets Work Suddenly?</h2><h3 id=regularization>Regularization</h3><p>see <a href=/posts/kbhregularization/>regularization</a></p><p>We want to be able to manipulate our parameters so that our models learn better&mdash;for instance, we want our weights to be low:</p><p>\begin{equation}
J_{L2}(\theta) = J_{reg}(\theta) + \lambda \sum_{k}^{} \theta^{2}_{k}
\end{equation}</p><p>or good &lsquo;ol dropout&mdash;&ldquo;fetaure dependent regularization&rdquo;</p><h4 id=motivation>Motivation</h4><ul><li><strong>classic view</strong>: regularization works to prevent <em>overfitting</em> when we have a lot of features</li><li><strong>NEW view with big models</strong>: regularization produces <em>generalizable</em> models when parameter count is big enough</li></ul><h3 id=dropout>Dropout</h3><p><a href=#dropout>Dropout</a>: prevents feature co-adaptation => results in good regularization</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_cs224n_apr182024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_cs224n_apr182024/>SU-CS224N APR182024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=perplexity--kbhperplexity-dot-md><a href=/posts/kbhperplexity/>perplexity</a></h2><p>see <a href=/posts/kbhperplexity/>perplexity</a></p><h2 id=vanishing-gradients>Vanishing Gradients</h2><p>Consider how an <a href=/posts/kbhlanguage_model/#recurrent-neural-network>RNN</a> works: as your sequence gets longer, the earlier layers gets very little gradients because you have to multiply the gradient of each layer by the other.</p><p>Alternatively, if the gradient is very large, the parameter updates can blow up exponentially as well if your weights are too large (its either exponentially small or exponentially huge).</p><h3 id=why-is-this-a-problem>Why is this a problem?</h3><p>To some extent, you can consider that we should tune the nearby weights a lot more than stuff way earlier than the sequence. Ham-fisting, we roughly have 7 tokens worth of effective conditioning.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_cs224n_apr232024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_cs224n_apr232024/>SU-CS224N APR232024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=evaluating-machine-translation>Evaluating Machine Translation</h2><h3 id=bleu>BLEU</h3><p>Compare machine vs. multiple-human reference translations. Uses <a href=/posts/kbhn_grams/>N-Gram</a> geometric mean&mdash;the actual n gram size isn&rsquo;t super special.</p><p>Original idea to have <strong>multiple reference translations</strong>&mdash;but maybe people to do this only one reference translation&mdash;good score <strong>in expectation</strong>.</p><h4 id=limitations>Limitations</h4><ul><li>good translation can get a bad BLEU because it has low n gram overlap</li><li>penalty to too-short system translations (i.e. translating only easy sentences isn&rsquo;t a good metric)</li><li>you really can&rsquo;t get to 100 in BLEU because of variations in text</li></ul><h2 id=attention>attention</h2><p>Given a vector of <strong>values</strong>, a vector <strong>query</strong>, attention is a technique to compute a weighted sum of the values depending on the query.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_cs224n_apr252024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_cs224n_apr252024/>SU-CS224N APR252024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=transformers--kbhtransformers-dot-md><a href=/posts/kbhtransformers/>Transformers</a></h2><h3 id=motivation>Motivation</h3><h4 id=lower-sequence-length-time-complexity>Lower Sequence-Length Time Complexity</h4><h4 id=minimize-linear-interaction-distance>Minimize Linear Interaction Distance</h4><p>The interaction distances scale by \(O(l)\) with \(l\) sequence length&mdash;gradient is affected by linear interaction distance: linear order is baked in.</p><h4 id=maximize-parallelization>Maximize Parallelization</h4><p>Forward and backward passes require waiting (waiting for it to roll from left to right)&mdash;-instead, you can compute attention in parallel.</p><h3 id=key-advantage>Key Advantage</h3><ol><li>Maximum interaction distance is \(O(1)\) &mdash; each word is connected to each other word</li><li>Unparallizable operation does not increase by sequence length</li></ol><h3 id=self-attention>Self-Attention</h3><p>Self-attention is formulated as each word in a sequence attending to each word in the same sequence.</p></span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/300/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/299/ aria-label="Page 299" class=page-link role=button>299</a></li><li class=page-item><a href=/posts/page/300/ aria-label="Page 300" class=page-link role=button>300</a></li><li class="page-item active"><a aria-current=page aria-label="Page 301" class=page-link role=button>301</a></li><li class=page-item><a href=/posts/page/302/ aria-label="Page 302" class=page-link role=button>302</a></li><li class=page-item><a href=/posts/page/303/ aria-label="Page 303" class=page-link role=button>303</a></li><li class=page-item><a href=/posts/page/302/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/367/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
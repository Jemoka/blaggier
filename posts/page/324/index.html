<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Posts</title>
<meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Posts</div></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><main><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_engr76_apr042024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_engr76_apr042024/>SU-ENGR76 APR042024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=information>information</h2><p><em>Information is the amount of surprise a message provides.</em></p><p>Shannon (1948): a mathematical theory for communication.</p><h3 id=information-value>information value</h3><p>The <a href=#information-value>information value</a>, or <a href=#information-value>entropy</a>, of a <a href=#information-source>information source</a> is its probability-weighted average <a href=#surprise>surprise</a> of all possible outcomes:</p><p>\begin{equation}
H(X) = \sum_{x \in X}^{} s(P(X=x)) P(X=x)
\end{equation}</p><h4 id=properties-of-entropy>properties of entropy</h4><ul><li><strong>entropy is positive</strong>: \(H(x) \geq 0\)</li><li><strong>entropy of uniform</strong>: for \(M \sim CatUni[1, &mldr;, n]\), \(p_{i} = \frac{1}{|M|} = \frac{1}{n}\), and \(H(M) = \log_{2} |M| = \log_{2} n\)</li><li><strong>entropy is bounded</strong>: \(0 \leq H(X) \leq H(M)\) where \(|X| = |M|\) and \(M \sim CatUni[1 &mldr; n]\) (&ldquo;uniform distribution has the highest entropy&rdquo;); we will reach the upper bound IFF \(X\) is uniformly distributed.</li></ul><h4 id=binary-entropy-function>binary entropy function</h4><p>For some binary outcome \(X \in \{1,2\}\), where \(P(x=1) = p_1\), \(P(X_2 = 2) = 1-p_1\). We can write:</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_engr76_apr092024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_engr76_apr092024/>SU-ENGR76 APR092024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=joint-entropy>joint entropy</h2><p>Suppose we have two <a href=/posts/kbhsu_engr76_apr042024/#information-source>information source</a>s; <a href=#joint-entropy>joint entropy</a> is the measure of joint surprise when we are defined over more than one <a href=/posts/kbhsu_engr76_apr042024/#information-source>information source</a>.</p><p>For a pair of random variables, \(X, Y\), their <a href=#joint-entropy>joint entropy</a> is defined over a new random variable of \(X \cup Y\).</p><p>\begin{equation}
H(x,y) = \sum_{i \in X}^{} \sum_{j \in Y}^{} P(X=i, Y=j) \log_{2} \qty(\frac{1}{P(X=i, Y=j)})
\end{equation}</p><p>If \(X \perp Y\), we can write \(H(x,y) = H(x)+H(y)\) (shown below.) Further, we have for all \(X\) and \(Y\), \(H(x,y) \leq H(x) + H(y)\) because you can never be more surprised than if you got two completely independent pieces of information.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_engr76_apr112024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_engr76_apr112024/>SU-ENGR76 APR112024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><p>We want a way of generating a <a href=/posts/kbhsu_engr76_apr092024/#prefix-free>Prefix-Free</a> code for any <a href=/posts/kbhsu_engr76_apr092024/#source-coding>Source Coding</a> problem.</p><h2 id=huffman-coding--kbhhuffman-coding-dot-md><a href=/posts/kbhhuffman_coding/>Huffman Coding</a></h2><p>See <a href=/posts/kbhhuffman_coding/>Huffman Coding</a></p><h2 id=diadic-source>Diadic Source</h2><p><a href=#diadic-source>Diadic Source</a> is an <a href=/posts/kbhsu_engr76_apr042024/#information-source>information source</a> for which the probability of occurrence for each symbol is a member of \(\frac{1}{2^{n}}\) for integer \(n\).</p><p>That is, the probability of each symbol is in powers of \(\frac{1}{2}\).</p><p>In THESE sources, <a href=/posts/kbhhuffman_coding/>Huffman Coding</a> will always result in a code that communicates the information in the same number of bits as the <a href=/posts/kbhsu_engr76_apr042024/#information-value>entropy</a> of the source.</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_engr76_apr162024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_engr76_apr162024/>SU-ENGR76 APR162024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=non-iid-sequence-can-have-smaller-entropy>Non-IID Sequence Can Have Smaller Entropy</h2><p>For sequences that are not <a href=/posts/kbhindependently_and_identically_distributed/>IID</a>, we may have:</p><p>\begin{equation}
H(X_1, \dots, X_{n)} \ll \sum_{j=1}^{n} H(X_{j})
\end{equation}</p><p>This means that for very dependent sequences:</p><p>\begin{equation}
\lim_{n \to \infty} \frac{H(X_1, \dots, X_{n})}{n} \ll \sum_{j=1}^{n}H(x_{j})
\end{equation}</p><p>so to measure how good our compression is, we should use this.</p><h2 id=signal>signal</h2><p>a <a href=#signal>signal</a> is, mathematically, just a function.</p><p>\begin{equation}
f: \mathbb{R}^{n} \to \mathbb{R}^{m}
\end{equation}</p><p>whereby the input is space (time, coordinates, etc.) and the output is the &ldquo;signal&rdquo; (pressure, level of gray, RGB, etc.)</p></span></div><div class=pageview onclick='window.location.href="https://www.jemoka.com/posts/kbhsu_engr76_apr182024/"'><span class=top><h1><a class=noannot style=cursor:pointer href=https://www.jemoka.com/posts/kbhsu_engr76_apr182024/>SU-ENGR76 APR182024</a></h1><span class="modbox right">Last edited: <span class=moddate>August 8, 2025</span></span>
</span><span class=summary><h2 id=fourier-series--kbhfourier-series-dot-md--as-exactly-a-shifted-sum-of-sinusoids><a href=/posts/kbhfourier_series/>Fourier Series</a> as exactly a shifted sum of sinusoids</h2><p>Key idea: <strong>every periodic function with period \(L\) can be represented as a sum of sinusoids</strong></p><p>\begin{equation}
f(t) = A_0 + \sum_{i=1}^{\infty} B_{j} \sin \qty(k \omega t + \phi_{j})
\end{equation}</p><p>where \(\omega = \frac{2\pi}{T}\). notice! without the \(A_0\) shift, our thing would integrate to \(0\) for every \(L\); hence, to bias the mean, we change \(A_0\).</p><p>Now, we ideally really want to get rid of that shift term \(\phi\), applying the sin sum formula:</p></span></div><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>⟸</span></a></li><li class=page-item><a href=/posts/page/323/ aria-label=Previous class=page-link role=button><span aria-hidden=true>⟵</span></a></li><li class=page-item><a href=/posts/page/322/ aria-label="Page 322" class=page-link role=button>322</a></li><li class=page-item><a href=/posts/page/323/ aria-label="Page 323" class=page-link role=button>323</a></li><li class="page-item active"><a aria-current=page aria-label="Page 324" class=page-link role=button>324</a></li><li class=page-item><a href=/posts/page/325/ aria-label="Page 325" class=page-link role=button>325</a></li><li class=page-item><a href=/posts/page/326/ aria-label="Page 326" class=page-link role=button>326</a></li><li class=page-item><a href=/posts/page/325/ aria-label=Next class=page-link role=button><span aria-hidden=true>⟶</span></a></li><li class=page-item><a href=/posts/page/367/ aria-label=Last class=page-link role=button><span aria-hidden=true>⟹</span></a></li></ul></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
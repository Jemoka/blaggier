<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Language Model Agents</title>
<meta name=description content="agents that uses the language to act on behave of another person or group.
Challenges
See Challenges of Language Model Agents
Methods
ReAct
See ReAct
Aguvis
Take the AgentNet dataset, and then tune a vison LM to roll out the rest of the sequence of actions given screenshots as input on top of a Qwen base model.
We can also add on top Chain of Thought to get more thinking as well.
Formulations
OSWorld
A unified task setup and evaluation."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Language Model Agents</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p>agents that uses the language to act on behave of another person or group.</p><h2 id=challenges>Challenges</h2><p>See <a href=/posts/kbhchallenges_of_language_model_agents/>Challenges of Language Model Agents</a></p><h2 id=methods>Methods</h2><h3 id=react>ReAct</h3><p>See <a href>ReAct</a></p><h3 id=aguvis>Aguvis</h3><p>Take the <a href=/posts/kbhagentnet/>AgentNet</a> dataset, and then tune a vison LM to roll out the rest of the sequence of actions given screenshots as input on top of a Qwen base model.</p><p>We can also add on top <a href=/posts/kbhchain_of_thought/>Chain of Thought</a> to get more thinking as well.</p><h2 id=formulations>Formulations</h2><h3 id=osworld>OSWorld</h3><p>A unified task setup and evaluation.</p><p>Motivation: Given language is a universal task specification, can we create a <strong>universal digital environment</strong>&mdash;with unified observation and action spaces?</p><h4 id=spec>spec</h4><ul><li><p>config</p><ul><li>initial state: how to setup, what to open, what files, etc.</li><li>evaluator: an evaluation script for task being done</li></ul></li></ul><ul><li><p>Obeservation</p><p>screen, screen shot, etc.</p><ul><li><p>Screenshot vs API Tradeoffs</p><ul><li>most websites/applications don&rsquo;t have them exposed</li><li>API outputs is very hard to verify quickly, whereas actual mouse action is easy to verify+stop</li></ul></li></ul></li></ul><ul><li><p>action</p><p>mouse keyboard controls, move, PyAutoGui style</p></li></ul><h4 id=dataset>dataset</h4><p>369 computer use task for evaluations</p><h4 id=evals>evals</h4><p>Claude, for one, is still really really bad at computer use. Claude computer use gets ~20% success rate versus humans&rsquo; ~70%.</p><h3 id=interactive-agents>Interactive Agents</h3><p>Big question: how to we align agents in an interactive, dynamic way (i.e. without instruction fine tuning which is hard). Language is information that helps agents <strong>predict the future</strong>; instructions is <strong>world modeling</strong></p><ul><li>instead of instructions => actions (executor)</li><li>instructions => updated belief (world model)</li></ul><p>User intent => action shouldn&rsquo;t have LLM language representation in the middle as a bottleneck.</p><p>There is an underlying representation of the user&rsquo;s preferences, you have to use language to coax it out of them.</p><h4 id=dynalang>Dynalang</h4><ol><li>build model that takes vision + language as a joint input</li><li>pass it through an auto-encoding representation</li><li>have the world model predict the next-encoding representation</li></ol><p>Main Idea: modeling language/tokens/images as a joint latent representation over time.</p><p>Training objective:</p><ol><li>reconstruction loss against the future presentation: using \(R_{i}\) to predict \(R_{i+1}\)</li><li>predict the reward over time</li><li>regularize?</li></ol><ul><li><p>Workflow</p><ol><li>take reward/preferences/behavior data</li><li><a href=/posts/kbhstructure_learning/>structure learning</a> to create the relationships between elements in the data structure</li></ol></li></ul><h2 id=evaluations>Evaluations</h2><h3 id=computer-agent-arena>Computer Agent Arena</h3><p><a href=https://arena.xlang.ai>https://arena.xlang.ai</a></p><ul><li>an open source platform for digital ai agents</li><li>users can preference-rank different agent performances</li></ul><h4 id=workflow>workflow</h4><ul><li>select OS environment (Windows, Ubuntu&mldr;) to create identical instances</li><li>configure computers in <strong>initial setup</strong> using preset scripts / click to have custom setup (why custom setups? to create diversity of senarios to help more generalization)</li><li>we automatically generate interaction scenarios given a user task prompt</li><li>finally, human perform scoring:<ol><li>Correct or Not?</li><li>Which one is Better?</li><li>Safe or not?</li></ol></li></ul><h4 id=goals>goals</h4><ul><li>for eval: evaluate + rank agents</li><li>training: data collection, RL, etc.</li></ul></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
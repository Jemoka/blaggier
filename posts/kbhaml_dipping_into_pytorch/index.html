<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>AML: Dipping into PyTorch</title>
<meta name=description content="Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.
Getting Started
To get started, let&rsquo;s open a colab and import Torch!
import torch
import torch.nn as nn
The top line here import PyTorch generally, and the bottom line imports the Neural Network libraries. We will need both for today and into the future!"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><div style="padding:20px 0 30px"><div id=title><h1>AML: Dipping into PyTorch</h1></div><aside id=toc><h1 id=toc-title>table of contents</h1><nav id=TableOfContents><ul><li><a href=#getting-started>Getting Started</a></li><li><a href=#tensors-and-autograd>Tensors and AutoGrad</a><ul><li><a href=#your-first-tensors>Your First Tensors</a></li><li><a href=#connecting-tensors>Connecting Tensors</a></li><li><a href=#autograd>Autograd</a></li><li><a href=#now-do-that-a-lot-of-times-dot>Now do that a lot of times.</a></li><li><a href=#so-why-the-heck-are-we-doing-all-this>So why the heck are we doing all this</a></li></ul></li><li><a href=#y-mx-plus-b-and-your-first-neural-network-module>y=mx+b and your first neural network &ldquo;module&rdquo;</a><ul><li><a href=#nn-dot-linear><code>nn.Linear</code></a></li><li><a href=#nn-dot-module><code>nn.Module</code></a></li><li><a href=#how-to-train-your-neural-network>How to Train Your <del>Dragon</del> Neural Network</a></li></ul></li><li><a href=#challenge>Challenge</a></li></ul></nav></aside><main><article><div><p>Hello! Welcome to the series of guided code-along labs to introduce you to the basis of using the PyTorch library and its friends to create a neural network! We will dive deeply into Torch, focusing on how practically it can be used to build Neural Networks, as well as taking sideroads into how it works under the hood.</p><h2 id=getting-started>Getting Started</h2><p>To get started, let&rsquo;s open a <a href=https://colab.research.google.com/>colab</a> and import Torch!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span></code></pre></div><p>The top line here import PyTorch generally, and the bottom line imports the Neural Network libraries. We will need both for today and into the future!</p><h2 id=tensors-and-autograd>Tensors and AutoGrad</h2><p>The most basic element we will be working with in Torch is something called a <strong>tensor</strong>. A tensor is a <strong>variable</strong>, which holds either a single number (<strong>scalar</strong>, or a single <strong>neuron</strong>) or a list of numbers (<strong>vector</strong>, or a <strong>layer</strong> of neurons), that <em>can change</em>. We will see what that means in a sec.</p><h3 id=your-first-tensors>Your First Tensors</h3><p>Everything that you are going to put through to PyTorch needs to be in a tensor. Therefore, we will need to get good at making them! As we discussed, a tensor can hold an number (scalar), a list (vector) or a (matrix).</p><p>Here are a bunch of them!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>scalar_tensor</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>2.2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>vector_tensor</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span><span style=color:#ae81ff>4</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>matrix_tensor</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([[</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#ae81ff>4</span><span style=color:#111>],[</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#ae81ff>7</span><span style=color:#111>,</span><span style=color:#ae81ff>4</span><span style=color:#111>]])</span>
</span></span></code></pre></div><p>You can perform operations on these tensors, like adding them together:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>2.2</span><span style=color:#111>)</span> <span style=color:#f92672>+</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>5.1</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(7.3000)
</span></span></code></pre></div><p>Vector and Matrix tensors work like NumPy arrays. You can add them pairwise:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([[</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#ae81ff>4</span><span style=color:#111>],[</span><span style=color:#ae81ff>1</span><span style=color:#111>,</span><span style=color:#ae81ff>7</span><span style=color:#111>,</span><span style=color:#ae81ff>4</span><span style=color:#111>]])</span> <span style=color:#f92672>+</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([[</span><span style=color:#ae81ff>0</span><span style=color:#111>,</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span><span style=color:#ae81ff>1</span><span style=color:#111>],[</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span><span style=color:#ae81ff>4</span><span style=color:#111>]])</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([[ 3,  3,  5],
</span></span><span style=display:flex><span>        [ 4, 10,  8]])
</span></span></code></pre></div><h3 id=connecting-tensors>Connecting Tensors</h3><p>A single number can&rsquo;t be a neural network! ([citation needed]) So, to be able to actually build networks, we have to connect tensors together.</p><p>So, let&rsquo;s create two tensors, each holding a neuron, and connect them together!</p><p>Here are two lovely scalar tensors:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>3.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>var_2</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>4.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3., requires_grad=True), tensor(4., requires_grad=True))
</span></span></code></pre></div><p>We initialized two numbers, <code>3</code>, which we named <code>var_1</code>, and <code>4</code>, which we named <code>var_2</code>.</p><p>The value <code>requires_grad</code> here tells PyTorch that these values can change, which we need it to do&mldr; very shortly!</p><p>First, though, let&rsquo;s create a <strong>latent</strong> variable. A &ldquo;latent&rdquo; value is a value that is the <em>result</em> of operations on other non-latent tensors&mdash;connecting the activation of some neurons together with a new one. For instance, if I multiplied our two tensors together, we can create our very own latent tensor.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_latent_value</span> <span style=color:#f92672>=</span> <span style=color:#111>var_1</span><span style=color:#f92672>*</span><span style=color:#111>var_2</span>
</span></span><span style=display:flex><span><span style=color:#111>my_latent_value</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>Evidently, \(3 \cdot 4 = 12\).</p><h3 id=autograd>Autograd</h3><p>Now! The beauty of PyTorch is that we can tell it to set any particular latent variable to \(0\) (Why only \(0\), and \(0\) specifically? Calculus; turns out this limitation doesn&rsquo;t matter at all, as we will see), and it can update all of its constituent tensors with <code>required_grad</code> &ldquo;True&rdquo; such that the latent variable we told PyTorch to set to \(0\) indeed becomes \(0\)!</p><p>This process is called &ldquo;automatic gradient calculation&rdquo; and &ldquo;backpropagation.&rdquo; (Big asterisks throughout, but bear with us. Find Matt/Jack if you want more.)</p><p>To do this, we will leverage the help of a special optimization algorithm called <strong>stochastic gradient descent</strong>.</p><p>Let&rsquo;s get a box of this stuff first:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>torch.optim</span> <span style=color:#f92672>import</span> <span style=color:#111>SGD</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>SGD</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&lt;class &#39;torch.optim.sgd.SGD&#39;&gt;
</span></span></code></pre></div><p>Excellent. By the way, from the <code>torch.optim</code> package, there&rsquo;s tonnes (like at least 20) different &ldquo;optimizer&rdquo; algorithms that all do the same thing (&ldquo;take this latent variable to \(0\) by updating its constituents&rdquo;) but do them in important different ways. We will explore some of them through this semester, and others you can Google for yourself by looking up &ldquo;PyTorch optimizers&rdquo;.</p><p>Ok, to get this SGD thing up and spinning, we have to tell it every tensor it gets to play with in a list. For us, let&rsquo;s ask PyTorch SGD to update <code>var_1</code> and <code>var_2</code> such that <code>my_latent_value</code> (which, remember, is var1 times var2) becomes a new value.</p><hr><p>Aside: <strong>learning rate</strong></p><p>Now, if you recall the neural network simulation, our model does not reach the desired outcome immediately. It does so in <em>steps</em>. The size of these steps are called the <strong>learning rate</strong>; the LARGER these steps are, the quicker you will get <em>close</em> to your desired solution, but where you end up getting maybe farther away from the actual solution; and vise versa.</p><p>Think about the learning rate as a hoppy frog: a frog that can hop a yard at a time (&ldquo;high learning rate&rdquo;) can probably hit a target a mile away much quicker, but will have a hard time actually hitting the foot-wide target precisely; a frog that can hop an inch at a time (&ldquo;low learning rate&rdquo;) can probably hit a target a mile away&mldr;. years from now, but will definitely be precisely hitting the foot-wide target when it finally gets there.</p><p>So what does &ldquo;high&rdquo; and &ldquo;low&rdquo; mean? Usually, we adjust learning rate by considering the number of decimal places it has. \(1\) is considered a high learning rate, \(1 \times 10^{-3} = 0.001\) as medium-ish learning rate, and \(1 \times 10^{-5}=0.00001\) as a small one. There are, however, no hard and fast rules about this and it is subjcet to experimentation.</p><hr><p>So, choose also an appropriate <strong>learning rate</strong> for our optimizer. I would usually start with \(3 \times 10^{-3}\) and go from there. In Python, we write that as <code>3e-3</code>.</p><p>So, let&rsquo;s make a SGD, and give it <code>var_1</code> and <code>var_2</code> to play with, and set the learning rate to <code>3e-3</code>:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_sgd</span> <span style=color:#f92672>=</span> <span style=color:#111>SGD</span><span style=color:#111>([</span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#111>],</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>3e-3</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>my_sgd</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>SGD (
</span></span><span style=display:flex><span>Parameter Group 0
</span></span><span style=display:flex><span>    dampening: 0
</span></span><span style=display:flex><span>    differentiable: False
</span></span><span style=display:flex><span>    foreach: None
</span></span><span style=display:flex><span>    lr: 0.003
</span></span><span style=display:flex><span>    maximize: False
</span></span><span style=display:flex><span>    momentum: 0
</span></span><span style=display:flex><span>    nesterov: False
</span></span><span style=display:flex><span>    weight_decay: 0
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Wonderful. Don&rsquo;t worry much about how many of these means for now; however, we will see it in action shortly.</p><p>Now! Recall that we allowed <code>my_sgd</code> to mess with <code>var_1</code> and <code>var_2</code> to change the value of <code>my_latent_value</code> (the product of <code>var_1</code> and <code>var_2</code>).</p><p>Current, <code>var_1</code> and <code>var_2</code> carries the values of:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3., requires_grad=True), tensor(4., requires_grad=True))
</span></span></code></pre></div><p>And, of course, their product <code>my_latent_value</code> carries the value of:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_latent_value</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>What if we want <code>my_latent_value</code> to be&mldr; \(15\)? That sounds like a good number. Let&rsquo;s ask our SGD algorithm to update <code>var_1</code> and <code>var_2</code> such that <code>my_latent_value</code> will be \(15\)!</p><p>Waaait. I mentioned that the optimizers can only take things to \(0\). How could it take <code>my_latent_value</code> to \(15\) then? Recall! I said SGD takes <em>a</em> latent variable to \(0\). So, we can just build another latent variable such that, when <code>my_latent_value</code> is \(15\), our new latent variable will be \(0\), and then ask SGD optimize on that!</p><p>What could that be&mldr; Well, the <em>squared difference</em> between \(15\) and <code>my_latent_value</code> is a good one. If <code>my_latent_value</code> is \(15\), the <em>squared difference</em> between it and \(15\) will be \(0\), as desired!</p><p>So, similar to what we explored last semester, we use <strong>sum of squared difference</strong> as our <strong>loss</strong> because it will be able to account for errors of fit in both directions: a \(-4\) difference in predicted and actual output is just as bad as a \(+4\) difference.</p><p>Turns out, the &ldquo;objective&rdquo; for SGD optimization, the thing that we ask SGD to take to \(0\) on our behalf by updating the parameters we allowed it to update (again, they are <code>var_1</code> and <code>var_2</code> in our case here), is indeed the <strong>loss</strong> value of our model. <strong>Sum of squared errors</strong> is, therefore, called our <strong>loss function</strong> for this toy problem.</p><p>So let&rsquo;s do it! Let&rsquo;s create a tensor our loss:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#ae81ff>15</span><span style=color:#f92672>-</span><span style=color:#111>my_latent_value</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span><span style=color:#111>loss</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(9., grad_fn=&lt;PowBackward0&gt;)
</span></span></code></pre></div><p>Nice. So our loss is at \(3\) right now; when <code>my_latent_value</code> is correctly at \(15\), our loss will be at \(0\)! So, to get <code>my_latent_value</code> to \(15\), we will ask SGD to take <code>loss</code> to \(0\).</p><p>To do this, there are three steps. <strong>COMMIT THIS TO MEMORY</strong>, as it will be basis of literally everything else in the future.</p><ol><li>Backpropagate: &ldquo;please tell SGD to take this variable to \(0\), and mark the correct tensors to change&rdquo;</li><li>Optimize: &ldquo;SGD, please update the marked tensors such that the variable I asked you to take to \(0\) is closer to \(0\)&rdquo;</li><li>Reset: &ldquo;SGD, please get ready for step 1 again by unmarking everything that you have changed&rdquo;</li></ol><p>Again! Is it commited to memory yet?</p><ol><li>Backprop</li><li>Optimize</li><li>Reset</li></ol><p>I am stressing this here because a <em>lot</em> of people 1) miss one of these steps 2) do them out of order. Doing these in any other order will cause your desired result to not work. Why? Think about what each step does, and think about doing them out of order.</p><p>One more time for good luck:</p><ol><li>Backprop!</li><li>Optimize!</li><li>Reset!</li></ol><p>Let&rsquo;s do it.</p><h4 id=backprop>Backprop!</h4><p>Backpropergation marks the correct loss value to minimize (optimze towards being \(0\)), and marks all tensors with <code>requires_grad</code> set to True which make up the value of that loss value for update.</p><p>Secretly, this steps takes the <strong>partial derivative</strong> of our loss against each of the tensors we marked <code>requires_grad</code>, allowing SGD to &ldquo;slide down the gradient&rdquo; based on those partial derivatives. Don&rsquo;t worry if you didn&rsquo;t get that sentence.</p><p>To do this, we call <code>.backward()</code> on the loss we want to take to \(0\):</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>None
</span></span></code></pre></div><p>This call will produce nothing. And that&rsquo;s OK, because here comes&mldr;</p><h4 id=optimize>Optimize!</h4><p>The next step is tell SGD to update all of the tensors marked for update in the previous step to get <code>loss</code> closer to \(0\). To do this, we simply:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>None
</span></span></code></pre></div><p>This call will produce nothing. But, if you check now, the tensors should updated.</p><p>Although&mldr; You should&rsquo;t check! Because we have one more step left:</p><h4 id=reset>Reset!</h4><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>None
</span></span></code></pre></div><p>I cannot stress this enough. People often stop at the previous step because &ldquo;ooo look my tensors updated!!!&rdquo; and forget to do this step. THIS IS BAD. We won&rsquo;t go into why for now, but basically not resetting the update mark results in a tensor being updated twice, then thrice, etc. each time you call <code>.step()</code>, which will cause double-updates, which will cause you to overshoot (handwavy, but roughly), which is bad.</p><h4 id=ooo-look-my-tensors-updated>ooo look my tensors updated!!!</h4><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3.0720, requires_grad=True), tensor(4.0540, requires_grad=True))
</span></span></code></pre></div><p>WOAH! Look at that! Without us telling SGD, it figured out that <code>var_1</code> and <code>var_2</code> both need to be BIGGER for <code>my_latent_value</code>, the product of <code>var_1</code> and <code>var_2</code> to change from \(12\) to \(15\). Yet, the product of \(3.0720\) and \(4.0540\) is hardly close to \(15\).</p><p>Why? Because our step size. It was <em>tiny!</em> To get <code>my_latent_value</code> to be properly \(15\), we have to do the cycle of 1) calculating new latent value 2) calculating new loss 3) backprop, optimize, reset, a LOT of times.</p><h3 id=now-do-that-a-lot-of-times-dot>Now do that a lot of times.</h3><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#ae81ff>100</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_latent_value</span> <span style=color:#f92672>=</span> <span style=color:#111>var_1</span><span style=color:#f92672>*</span><span style=color:#111>var_2</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#ae81ff>15</span><span style=color:#f92672>-</span><span style=color:#111>my_latent_value</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span> <span style=color:#75715e># BACKPROP!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span> <span style=color:#75715e># OPTIMIZE!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span> <span style=color:#75715e># RESET!</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3.4505, requires_grad=True), tensor(4.3472, requires_grad=True))
</span></span></code></pre></div><p>Weird solution, but we got there! The product of these two values is indeed very close to \(15\)! Give yourself a pat on the back.</p><h3 id=so-why-the-heck-are-we-doing-all-this>So why the heck are we doing all this</h3><p>So why did we go through all the effort of like 25 lines of code to get two numbers to multiply to \(15\)? If you think about Neural Networks as a process of <em>function fitting</em>, we are essentially asking our very basic &ldquo;network&rdquo; (as indeed, the chain of tensors to build up to our latent value, then to compute our loss, <em>is</em> a network!) to achieve a measurable task (&ldquo;take the product of these numbers to \(15\)&rdquo;). Though the relationships we will be modeling in this class will be more complex than literal multiplication, it will be just using more fancy mechanics of doing the same thing&mdash;taking tensors values which was undesirable, and moving them to more desirable values to model our relationship.</p><h2 id=y-mx-plus-b-and-your-first-neural-network-module>y=mx+b and your first neural network &ldquo;module&rdquo;</h2><h3 id=nn-dot-linear><code>nn.Linear</code></h3><p>The power of neural networks actually comes when a BUNCH of numbers gets multiplied together, all at once! using&mldr; VECTORS and MATRICIES! Don&rsquo;t remember what they are? Ask your friendly neighborhood Matt/Jack.</p><p>Recall, a <strong>matrix</strong> is how you can transform a <strong>vector</strong> from one space to another. Turns out, the brunt of everything you will be doing involves asking SGD to move a bunch of matricies around (like we did before!) such that our input vector(s) gets mapped to the right place.</p><p>A <strong>matrix</strong>, in neural network world, is referred to as a <strong>linear layer</strong>. It holds a whole <em>series</em> of neurons, taking every single value of the input into account to producing a whole set of output. Because of this property, it is considered a <strong>fully connected layer</strong>.</p><p>Let&rsquo;s create such a fully-connected layer (matrix) in PyTorch! When you ask PyTorch to make a matrix for you, you use the <code>nn</code> sublibrary which we imported before. Furthermore, and this is confusing for many people who have worked with matricies before, you specify the <strong>input dimension first</strong>.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_matrix_var_1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span> <span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>my_matrix_var_1</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Linear(in_features=3, out_features=2, bias=True)
</span></span></code></pre></div><p><code>my_matrix_var_1</code> is a linear map from three dimensions to two dimensions; it will take a vector of three things as input and spit out a vector of two.</p><p>Note! Although <code>my_matrix_var_1</code> <em>is</em> a tensor under the hood just like <code>var_1</code>, we 1) didn&rsquo;t have to set default values for it 2) didn&rsquo;t have to mark it as <code>requires_grad</code>. This is because, unlike a raw Tensor which often does not require to be changed (such as, for instance, the input value, which you can&rsquo;t change), a matrix is basically ALWAYS a tensor that encodes the <strong>weights</strong> of a model we are working with&mdash;so it is always going to be something that we will ask SGD to change on our behalf.</p><p>So, since you are asking SGD to change it anyways, PyTorch just filled a bunch of random numbers in for you and set <code>requires_grad</code> on for you to <code>my_matrix_var_1</code>. If you want to see the actual underlying tensor, you can:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_matrix_var_1</span><span style=color:#f92672>.</span><span style=color:#111>weight</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Parameter containing:
</span></span><span style=display:flex><span>tensor([[-0.2634,  0.3729,  0.5019],
</span></span><span style=display:flex><span>        [ 0.2796,  0.5425, -0.4337]], requires_grad=True)
</span></span></code></pre></div><p>As you can see, we have indeed what we expect: a tensor containing a \(2\times 3\) matrix with <code>requires_grad</code> on filled with random values.</p><p>How do we actually optimize over this tensor? You can do all the shenanigans we did before and pass <code>my_matrix_var_1</code> to SGD, but this will <em>quickly</em> get unwieldy as you have more parameters. Remember how we had to give SVG a list of EVERYTHING it had to keep track of? <code>var_1</code> and <code>var_2</code> was simple enough, but what if we had to do <code>var_1.weight</code>, <code>var_2.weight</code>, <code>var_3.weight</code>&mldr; &mldr; &mldr; <em>ad nausium</em> for every parameter we use on our large graph? GPT3 has 1.5 billion parameters. Do you really want to type that?</p><p>No.</p><p>There is, of course, a better way.</p><h3 id=nn-dot-module><code>nn.Module</code></h3><p>This, by the way, is the standard of how a Neural Network is properly built from now on until the industry moves on from PyTorch. You will want to remember this.</p><p>Let&rsquo;s replicate the example of our previous 3=>2 dimensional linear map, but with a whole lot more code.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MyNetwork</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># important: runs early calls to make sure that</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># the module is correct</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we declare our layers. we will use them below</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># this is a special function that you don&#39;t actually call</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># manually, but as you use this module Torch will call</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># on your behalf. It passes the input through to the layers</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># of your network.</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we want to pass whatever input we get, named x</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># through to every layer. right now there is only</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># one fully-connected layer</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><p>What this does, behind the scenes, is to wrap our matrix and all of its parameters into one giant <strong>module</strong>. (NOTE! This is PyTorch-specific language. Unlike all other vocab before, this term is specific to PyTorch.) A module is an operation on tensors which can retain gradients (i.e. it can change, i.e. <code>requires_grad=True</code>).</p><p>Let&rsquo;s see it in action. Recall that our matrix takes a vector of 3 things as input, and spits out a vector of 2 things. So let&rsquo;s make a vector of three things:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>three_vector</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>1.</span><span style=color:#111>,</span><span style=color:#ae81ff>2.</span><span style=color:#111>,</span><span style=color:#ae81ff>3.</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>three_vector</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([1., 2., 3.])
</span></span></code></pre></div><p>By the way, notice the period I&rsquo;m putting after numbers here? That&rsquo;s a shorthand for <code>.0</code>. So <code>3.0 = 3.</code>. I want to take this opportunity to remind you that the tensor operations all take FLOATING POINT tensors as input, because the matrices themselves as initialized with random floating points.</p><p>Let&rsquo;s get an instance of the new <code>MyNetwork</code> module.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span> <span style=color:#f92672>=</span> <span style=color:#111>MyNetwork</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>my_network</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>MyNetwork(
</span></span><span style=display:flex><span>  (m1): Linear(in_features=3, out_features=2, bias=True)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>And apply this operation we designed to our three-vector!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>three_vector</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([0.3850, 1.4120], grad_fn=&lt;AddBackward0&gt;)
</span></span></code></pre></div><p>Woah! It mapped our vector tensor in three dimensions to a vector tensor in two!</p><p>The above code, by the way, is how we actually use our model to run <strong>predictions</strong>: <code>my_network</code> is <em>transforming</em> the input vector to the desired output vector.</p><p>Cool. This may not seem all that amazing to you&mldr; yet. But, remember, we can encode <em>any number</em> of matrix operations in our <code>forward()</code> function above. Let&rsquo;s design another module that uses two matricies&mdash;or two <strong>fully-connected layers</strong>, or <strong>layers</strong> for short (when we don&rsquo;t specify what kind of layer it is, it is fully connected)&mdash;to perform a transformation.</p><p>We will transform a vector from 3 dimensions to 2 dimensions, then from 2 dimensions to 5 dimensions:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MyNetwork</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># important: runs early calls to make sure that</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># the module is correct</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we declare our layers. we will use them below</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>3</span><span style=color:#111>,</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>2</span><span style=color:#111>,</span><span style=color:#ae81ff>5</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># this is a special function that you don&#39;t actually call</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># manually, but as you use this module Torch will call</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># on your behalf. It passes the input through to the layers</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># of your network.</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we want to pass whatever input we get, named x</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># through to every layer. right now there is only</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># one fully-connected layer</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><p>Of course, this network topology is kind of randomly tossed into the network.</p><p>Doing everything else we did before again, we should end up a vector in 5 dimensions, having been transformed twice behind the scenes!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span> <span style=color:#f92672>=</span> <span style=color:#111>MyNetwork</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>my_network</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>MyNetwork(
</span></span><span style=display:flex><span>  (m1): Linear(in_features=3, out_features=2, bias=True)
</span></span><span style=display:flex><span>  (m2): Linear(in_features=2, out_features=5, bias=True)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>And apply this operation we designed to our three-vector!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>three_vector</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([ 0.8241, -0.1014,  0.2940, -0.2019,  0.6749], grad_fn=&lt;AddBackward0&gt;)
</span></span></code></pre></div><p>Nice.</p><p>And here&rsquo;s the magical thing: when we are asking SGD to optimize this network, instead of needing to pass every darn parameter used in this network into SVG, we can just pass in:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&lt;generator object Module.parameters at 0x115214270&gt;
</span></span></code></pre></div><p>This is actually a list of every single <code>tensor</code> that has <code>requires_grad=True</code> that we secretly created. No more typing out a list of every parameter to SGD like we did with <code>var_1</code> and <code>var_2</code>! We will see this in action shortly.</p><h3 id=how-to-train-your-neural-network>How to Train Your <del>Dragon</del> Neural Network</h3><p>Note, the <code>MyNetwork</code> transformation is currently kind of useless. We know it maps the vector <code>[1,2,3]</code> to some arbitrary numbers above (i.e. <code>0.8241</code> an such). That&rsquo;s quite lame.</p><p>We want our network to model some relationship between numbers, that&rsquo;s why we are here. Let&rsquo;s, arbitrarily and for fun, ask SGD to update <code>my_network</code> such that it will return <code>[1,2,3,4,5]</code> given <code>[1,2,3]</code>.</p><p>By the way, from here on, I will use <code>MyNetwork</code> to refer to the model 3=>2=>5 network we made above generally, and <code>my_network</code> the specific <em>instantiation</em> of <code>MyNetwork</code> whose parameters we will ask SGD to update.</p><p>Let&rsquo;s get a clean copy of <code>MyNetwork</code> first:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span> <span style=color:#f92672>=</span> <span style=color:#111>MyNetwork</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>my_network</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>MyNetwork(
</span></span><span style=display:flex><span>  (m1): Linear(in_features=3, out_features=2, bias=True)
</span></span><span style=display:flex><span>  (m2): Linear(in_features=2, out_features=5, bias=True)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>And, let&rsquo;s create a <em>static</em> (i.e. SGD cannot change it) input and output vector pair which we will pass into our operation:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>1.</span><span style=color:#111>,</span><span style=color:#ae81ff>2.</span><span style=color:#111>,</span><span style=color:#ae81ff>3.</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span><span style=color:#111>my_desired_output</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>([</span><span style=color:#ae81ff>1.</span><span style=color:#111>,</span><span style=color:#ae81ff>2.</span><span style=color:#111>,</span><span style=color:#ae81ff>3.</span><span style=color:#111>,</span><span style=color:#ae81ff>4.</span><span style=color:#111>,</span><span style=color:#ae81ff>5.</span><span style=color:#111>])</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>my_input</span><span style=color:#111>,</span><span style=color:#111>my_desired_output</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor([1., 2., 3.]), tensor([1., 2., 3., 4., 5.]))
</span></span></code></pre></div><p>We will pass our input through the <code>my_network</code> operation, and figure out what our inputs currently map to:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network_output</span> <span style=color:#f92672>=</span> <span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>my_input</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>my_network_output</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([-1.4672, -0.7089, -0.2645, -0.0598,  0.1239], grad_fn=&lt;AddBackward0&gt;)
</span></span></code></pre></div><p>Ah, clearly not <code>[1,2,3,4,5]</code>. Recall we want these values to be the same as <code>my_output</code>, which they isn&rsquo;t doing right now. Let&rsquo;s fix that.</p><p>Can you guess what loss function we will use? &mldr; That&rsquo;s right, the same exact thing as before! Squaring the difference.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>(</span><span style=color:#111>my_network_output</span><span style=color:#f92672>-</span><span style=color:#111>my_desired_output</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span><span style=color:#111>loss</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([ 6.0869,  7.3380, 10.6571, 16.4821, 23.7766], grad_fn=&lt;PowBackward0&gt;)
</span></span></code></pre></div><p>Waiiiit. There&rsquo;s a problem. Remember, SGD can take a single latent value to \(0\). That&rsquo;s a whole lotta latent values in a vector! Which one will it take to \(0\)? Stop to think about this for a bit: we <em>want</em> to take all of these values to \(0\), but we can take only a single value to \(0\) with SGD. How can we do it?</p><p>To do this, we just&mldr; add the values up using the <code>torch.sum</code> function!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>((</span><span style=color:#111>my_network_output</span><span style=color:#f92672>-</span><span style=color:#111>my_desired_output</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>loss</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(64.3406, grad_fn=&lt;SumBackward0&gt;)
</span></span></code></pre></div><p>Nice. We now have something to optimize against, let&rsquo;s actually create our optimizer! Remember that, instead of passing in every single parameter we want PyTorch to change manually, we just pass in <code>my_network.parameters()</code> and PyTorch will scan for every single parameter that lives in <code>MyNetwork</code> and give it all to SGD:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_sgd</span> <span style=color:#f92672>=</span> <span style=color:#111>SGD</span><span style=color:#111>(</span><span style=color:#111>my_network</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>my_sgd</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>SGD (
</span></span><span style=display:flex><span>Parameter Group 0
</span></span><span style=display:flex><span>    dampening: 0
</span></span><span style=display:flex><span>    differentiable: False
</span></span><span style=display:flex><span>    foreach: None
</span></span><span style=display:flex><span>    lr: 1e-06
</span></span><span style=display:flex><span>    maximize: False
</span></span><span style=display:flex><span>    momentum: 0
</span></span><span style=display:flex><span>    nesterov: False
</span></span><span style=display:flex><span>    weight_decay: 0
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Just for running this model, we are going to run our network with more steps (\(50,000\)), but with smaller step sizes (\(1 \times 10^{-6}\)). We will not worry about it too much for now, and dive into discussing it further for network parameter tuning.</p><p>So, let&rsquo;s make the actual training loop now that will take the latent variable named <code>my_network_output</code>, created by applying <code>my_network</code> on <code>my_input</code>, to take on the value of <code>my_desired_output</code>! Can you do it without looking? This will be <em>almost</em> the same as our first training loop, except we are asking our network to calculate the current latent output (instead of computing it from scratch each time.)</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#ae81ff>50000</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># calculate new latent variable</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_network_output</span> <span style=color:#f92672>=</span> <span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>my_input</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># calculate loss</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>((</span><span style=color:#111>my_network_output</span><span style=color:#f92672>-</span><span style=color:#111>my_desired_output</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Backprop!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Optimize!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Reset!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>my_input</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([-0.9814,  0.4252,  1.8085,  2.7022,  3.5517], grad_fn=&lt;AddBackward0&gt;)
</span></span></code></pre></div><p>Not great! But&mdash;we are both <em>ordered</em> correctly and &mdash; if you just kept running this loop, we will eventually <strong>converge</strong> (arrive at) the right answer! For kicks, let&rsquo;s run it \(50000\) more times:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>for</span> <span style=color:#111>_</span> <span style=color:#f92672>in</span> <span style=color:#111>range</span><span style=color:#111>(</span><span style=color:#ae81ff>50000</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># calculate new latent variable</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_network_output</span> <span style=color:#f92672>=</span> <span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>my_input</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># calculate loss</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>((</span><span style=color:#111>my_network_output</span><span style=color:#f92672>-</span><span style=color:#111>my_desired_output</span><span style=color:#111>)</span><span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Backprop!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>loss</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Optimize!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Reset!</span>
</span></span><span style=display:flex><span>    <span style=color:#111>my_sgd</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>my_input</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([0.9975, 1.9986, 3.0006, 4.0026, 5.0052], grad_fn=&lt;AddBackward0&gt;)
</span></span></code></pre></div><p>Would you look at that! What did I promise you :)</p><p>Your network <em>learned</em> something! Specifically, the skill of mapping \([1,2,3]\) to \([1,2,3,4,5]\)! Congrats!</p><h2 id=challenge>Challenge</h2><p>Now that you know how to get the network to map a specific vector in three dimensions to a specific place in five dimensions, can you do that more generally? Can you generate and give your own network enough examples such that it will learn to do that for ALL vectors in three dimensions?</p><p>Specifically, generate a training set of in python and train your neural network now to perform the following operation:</p><p>Given a vector \([a,b,c]\), return \([a,b,c,c+1,c+2]\), for every integer \([a,b,c]\).</p><p>Hint: pass in many examples for correct behavior sequentially during each of your training loops, calculating loss and running the <strong>optimization step</strong> (i.e. back! optimize! reset!) after each example you give.</p></div></article></main></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
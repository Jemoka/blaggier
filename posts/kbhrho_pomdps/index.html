<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>rho-POMDPs</title>
<meta name=description content="POMDPs to solve Active Sensing Problem: where gathering information is the explicit goal and not a means to do something. Meaning, we can&rsquo;t train them using state-only reward functions (i.e. reward is based on belief and not state).
Directly reward the reduction of uncertainty: belief-based reward framework which you can just tack onto the existing solvers.
To do this, we want to define some reward directly over the belief space which assigns rewards based on uncertainty reduction:"><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>rho-POMDPs</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p><a href=/posts/kbhpartially_observable_markov_decision_process/>POMDP</a>s to solve <a href=/posts/kbhrho_pomdps/>Active Sensing Problem</a>: where <strong>gathering information</strong> is the explicit goal and not a means to do something. Meaning, we can&rsquo;t train them using state-only reward functions (i.e. reward is based on belief and not state).</p><p>Directly reward the <strong>reduction of uncertainty</strong>: <a href=/posts/kbhbelief/>belief</a>-based reward framework which you can just tack onto the existing solvers.</p><p>To do this, we want to define some reward directly over the belief space which assigns rewards based on uncertainty reduction:</p><p>\begin{equation}
r(b,a) = \rho(b,a)
\end{equation}</p><p>\(\rho\) should be some measure of uncertainty, like entropy.</p><p><strong>key question</strong>: how does our <a href=/posts/kbhpartially_observable_markov_decision_process/>POMDP</a> formulations change given this change?</p><h2 id=don-t-worry-about-the-value-function>Don&rsquo;t worry about the Value Function</h2><p>result: if <strong>reward function</strong> is convex, then Bellman updates should <strong>preserve the convexity of the value function</strong></p><p>So, we now just need to make sure that however we compute our rewards the reward function \(\rho\) has to be piecewise linear convex.</p><h2 id=pwlc--kbhrho-pomdps-dot-md--rewards><a href=/posts/kbhrho_pomdps/>PWLC</a> rewards</h2><p>One simple <a href=/posts/kbhrho_pomdps/>PWLC</a> rewards are <a href=/posts/kbhalpha_vector/>alpha vector</a>s:</p><p>\begin{equation}
\rho(b,a) = \max_{\alpha in \Gamma} \qty[\sum_{ss}^{} b(s) \alpha(s)]
\end{equation}</p><p>We want to use \(R\) extra alpha-vectors to compute the value at a state.</p><p>This makes our Belman updates:</p><figure><img src=/ox-hugo/2024-02-25_19-56-49_screenshot.png></figure><h2 id=non-pwlc--kbhrho-pomdps-dot-md--objectives>non-<a href=/posts/kbhrho_pomdps/>PWLC</a> objectives</h2><p>As long as \(\rho\) is convex and stronger-than <a href=/posts/kbhuniqueness_and_existance/#lipschitz-condition>Lipschitz continuous</a>, we can use a modified version of the Bellman updates to force our non <a href=/posts/kbhrho_pomdps/>PWLC</a> \(\rho\) into pretty much PWLC:</p><p>\begin{equation}
\hat{\rho}(b) = \max_{b&rsquo;} \qty[\rho(b&rsquo;) + (b-b&rsquo;) \cdot \nabla p(b&rsquo;)]
\end{equation}</p><p>Taylor never fails to disappoint.</p><p>Fancy math gives that the error in this would be bounded:</p><figure><img src=/ox-hugo/2024-02-25_19-59-10_screenshot.png></figure></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
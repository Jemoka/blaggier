<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Getting Started with PyTorch</title>
<meta name=description content="(Py)Torch is a great C++/Python library to construct and train complex neural networks. It has taken over academia over the last few years and is slowly taking over industry. Let&rsquo;s learn about how it works!
This document is meant to be read cover-to-cover. It makes NO SENSE unless read like that. I focus on building intuition about why PyTorch works, so we will be writing unorthodox code until the very end where we put all ideas together."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><div style="padding:20px 0 30px"><main><article><div><p>(Py)Torch is a great C++/Python library to construct and train complex neural networks. It has <a href=https://paperswithcode.com/trends>taken over academia</a> over the last few years and is slowly taking over industry. Let&rsquo;s learn about how it works!</p><p><strong><strong>This document is meant to be read cover-to-cover. It makes NO SENSE unless read like that. I focus on building intuition about why PyTorch works, so we will be writing unorthodox code until the very end where we put all ideas together.</strong></strong></p><p>The chapters below take you through large chapters in a machine-learning journey. But, to do anything, we need to import some stuff which we will need:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>numpy</span> <span style=color:#00a8c8>as</span> <span style=color:#111>np</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch</span>
</span></span></code></pre></div><h2 id=autograd>Autograd</h2><p><a href=https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html>source</a></p><p>I believe that anybody learning a new ML framework should learn how its differentiation tools work. Yes, this means that we should first understand how it works with not a giant matrix, but with just two simple variables.</p><p>At the heart of PyTorch is the built-in gradient backpropagation facilities. To demonstrate this, let us create two such variables.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>3.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>var_2</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>tensor</span><span style=color:#111>(</span><span style=color:#ae81ff>4.0</span><span style=color:#111>,</span> <span style=color:#111>requires_grad</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>(</span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>(tensor(3., requires_grad=True), tensor(4., requires_grad=True))
</span></span></code></pre></div><p>There is secretly a lot going on here, so let&rsquo;s dive in. First, just to get the stickler out of the way, <code>torch.tensor</code> (used here) is the generic variable creator, <code>torch.Tensor</code> (capital!) initializes a proper tensor&mdash;which you will <strong>never</strong> need.</p><p>What is a <code>tensor</code>? A <code>tensor</code> is simply a very efficient matrix that can updates its own values dynamically but keep the same variable name. The above commands creates two such <code>tensor</code>, both being <code>1x1</code> matrices.</p><p>Note that, for the initial values, I used <em>floats!</em> instead of <em>ints</em>. The above code will crash if you use ints: this is because we want the surface on which the matrix changes value to be smooth to make things like gradient descent to work.</p><p>Lastly, we have an argument <code>requires_grad=True</code>. This argument tells PyTorch to keep track of the gradient of the <code>tensor</code>. For now, understand this as &ldquo;permit PyTorch to change this variable if needed.&rdquo; More on that in a sec.</p><p>Naturally, if we have two tensors, we would love to multiply them!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_mult</span> <span style=color:#f92672>=</span> <span style=color:#111>var_1</span><span style=color:#f92672>*</span><span style=color:#111>var_2</span>
</span></span><span style=display:flex><span><span style=color:#111>var_mult</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>Wouldyalookatthat! Another tensor, with the value \(12\).</p><p>Now. Onto the main event. Back-Propagation! The core idea of a neural network is actually quite simple: figure out how much each input parameter (for us <code>var_1</code>, <code>var_2</code>) influence the output, then adjust the inputs accordingly to get the output to be \(0\).</p><p>To see what I mean, recall our output <code>tensor</code> named:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_mult</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(12., grad_fn=&lt;MulBackward0&gt;)
</span></span></code></pre></div><p>How much does changing <code>var_1</code> and <code>var_2</code>, its inputs, influence this output <code>tensor</code>? This is not immediately obvious, so let&rsquo;s write what we are doing out:</p><p>\begin{equation}
v_1 \cdot v_2 = v_{m} \implies 3 \cdot 4 = 12
\end{equation}</p><p>with \(v_1\) being <code>var_1</code>, \(v_2\) being <code>var_2</code>, and \(v_{m}\) being <code>var_mult</code>.</p><p>As you vary <code>var_1</code>, by <strong>what factor</strong> does the output change? For instance, if <code>var_1</code> (the \(3\)) suddenly became a \(2\), how much <em>less</em> will <code>var_mult</code> be? Well, \(2\cdot 4=8\), the output is exactly \(4\) less than before less than before. Hence, <code>var_1</code> influences the value of <code>var_mult</code> by a factor of \(4\); meaning every time you add/subtract \(1\) to the value of <code>var_1</code>, <code>var_mult</code> gets added/subtracted by a value of \(4\).</p><p>Similarly, as you vary <code>var_2</code>, by what factor does the output change? For instance, if <code>var_2</code> (the \(4\)) suddenly became a \(5\), how much <em>less</em> will <code>var_mult</code> be? Well, \(3\cdot 3=5\), the output is exactly \(3\) more than before less than before. Hence, <code>var_2</code> influences the value of <code>var_mult</code> by a factor of \(3\); meaning every time you add/subtract \(1\) to the value of <code>var_3</code>, <code>var_mult</code> gets added/subtracted by a value of \(3\).</p><p>Those of you who have exposure to Multi-Variable Calculus&mdash;this is indeed the same concept as a partial derivative of <code>var_mult</code> w.r.t. <code>var_1</code> and <code>var_2</code> for the previous two paragraphs respectively.</p><p>These relative-change-units (\(4\) and \(3\)) are called <strong>gradients</strong>: the factor by which changing any given variable change the output.</p><p>Now, gradient calculation is awfully manual! Surely we don&rsquo;t want to keep track of these tiny rates-of-change ourselves! This is where PyTorch autograd comes in. Autograd is the automated tool that helps you figure out these relative changes! It is built in to all PyTorch tensors.</p><p>In the previous paragraphs, we figured out the relative influences <code>var_1</code> and <code>var_2</code> on <code>var_multi</code>. Now let&rsquo;s ask a computer to give us the same result, in much less time.</p><p>First, we will ask PyTorch to calculate gradients for all variables that contributed to <code>var_mult</code>.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_mult</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>The <code>backward</code> function is a magical function that finds and calculates these relative-change-values of <code>var_multi</code> with respect to every variable that contributed to its values. To view the actual relative values, we will use <code>.grad</code> now on the actual variables:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_1</span><span style=color:#f92672>.</span><span style=color:#111>grad</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(4.)
</span></span></code></pre></div><p>Recall! We used our big brains to deduce above that changing <code>var_1</code> by \(1\) unit will change <code>var_mult</code> by \(4\) units. So this works!</p><p>The other variables works as expected:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>var_2</span><span style=color:#f92672>.</span><span style=color:#111>grad</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor(3.)
</span></span></code></pre></div><p>Yayyy! Still what we expected.</p><h2 id=gradient-descent>Gradient Descent</h2><p>Relative changes are cool, but it isn&rsquo;t all that useful unless we are actually doing some changing. We want to use our epic knowledge about the relative influences of <code>var_1</code> and <code>var_2</code>, to manipulate those variables such that <code>var_mult</code> is the value we want.</p><p><strong><strong><strong><strong>THE REST OF THIS DOCUMENT IS IN CONSTRUCTION</strong></strong></strong></strong></p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.optim</span> <span style=color:#00a8c8>as</span> <span style=color:#111>optim</span>
</span></span></code></pre></div><p>To start an optimizer, you give it all the variables for which it should keep track of updating.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>SGD</span><span style=color:#111>([</span><span style=color:#111>var_1</span><span style=color:#111>,</span> <span style=color:#111>var_2</span><span style=color:#111>],</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span><span style=color:#111>,</span> <span style=color:#111>momentum</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>And then, to update gradients, you just have to:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>step</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#75715e># IMPORTANT</span>
</span></span><span style=display:flex><span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>zero_grad</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>What&rsquo;s that <code>zero_grad</code>? That clears the gradients from the variables (after applying them with <code>.step()</code>) so that the next update doesn&rsquo;t influence the current one.</p><h2 id=your-first-neural-network>Your First Neural Network</h2><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>torch.nn</span> <span style=color:#00a8c8>as</span> <span style=color:#111>nn</span>
</span></span></code></pre></div><h3 id=layers>Layers</h3><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>m</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>m</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span><span style=color:#111>,</span> <span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>size</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>Explain what the \(20, 30\) means.</p><p>Ok one layer is just lame. What if you want a bunch of layers?</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m3</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>40</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># function call syntax! Functions call from rigth to left!</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>m3</span><span style=color:#111>(</span><span style=color:#111>m2</span><span style=color:#111>(</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)))</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span><span style=color:#111>,</span> <span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>size</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>And guess what? If you want to adjust the values here, you would just do:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>m3</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span> <span style=color:#ae81ff>40</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># function call syntax! Functions call from rigth to left!</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>m3</span><span style=color:#111>(</span><span style=color:#111>m2</span><span style=color:#111>(</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)))</span>
</span></span><span style=display:flex><span><span style=color:#111>(</span><span style=color:#111>output</span><span style=color:#f92672>.</span><span style=color:#111>sum</span><span style=color:#111>()</span> <span style=color:#f92672>-</span> <span style=color:#ae81ff>12</span><span style=color:#111>)</span><span style=color:#f92672>.</span><span style=color:#111>backward</span><span style=color:#111>()</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>None
</span></span></code></pre></div><p>But wait! What are the options you give to your optimizer?</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>SGD</span><span style=color:#111>([</span><span style=color:#111>m1</span><span style=color:#f92672>.</span><span style=color:#111>weight</span><span style=color:#111>,</span> <span style=color:#111>m1</span><span style=color:#f92672>.</span><span style=color:#111>bias</span> <span style=color:#f92672>...</span> <span style=color:#f92672>...</span> <span style=color:#111>],</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span><span style=color:#111>,</span> <span style=color:#111>momentum</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>That&rsquo;s a <em>lot of variables!!</em> Each linear layer has a \(m\) and a \(b\) (from \(y=mx+b\) fame), and you will end up with a bajillon one of those! Also, that function call syntax, chaining one layer after another, is so knarly! Can we do better? Yes.</p><h3 id=an-honest-to-goodness-neural-network>An Honest-to-Goodness Neural Network</h3><p>PyTorch makes the <code>module</code> framework to make model creator&rsquo;s lives easier. This is the best practice for creating a neural network.</p><p>Let&rsquo;s replicate the example above with the new <code>module</code> framework:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#00a8c8>class</span> <span style=color:#75af00>MyNetwork</span><span style=color:#111>(</span><span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Module</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#111>__init__</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># important: runs early calls to make sure that</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># the module is correct</span>
</span></span><span style=display:flex><span>        <span style=color:#111>super</span><span style=color:#111>()</span><span style=color:#f92672>.</span><span style=color:#111>__init__</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we declare our layers. We don&#39;t use them yet.</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>20</span><span style=color:#111>,</span><span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m2</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span><span style=color:#ae81ff>30</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m3</span> <span style=color:#f92672>=</span> <span style=color:#111>nn</span><span style=color:#f92672>.</span><span style=color:#111>Linear</span><span style=color:#111>(</span><span style=color:#ae81ff>30</span><span style=color:#111>,</span><span style=color:#ae81ff>40</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># this is a special function that is called when</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># the module is called</span>
</span></span><span style=display:flex><span>    <span style=color:#00a8c8>def</span> <span style=color:#75af00>forward</span><span style=color:#111>(</span><span style=color:#111>self</span><span style=color:#111>,</span> <span style=color:#111>x</span><span style=color:#111>):</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># we want to pass our input through to every layer</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># like we did before, but now more declaritively</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m1</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m2</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>        <span style=color:#111>x</span> <span style=color:#f92672>=</span> <span style=color:#111>self</span><span style=color:#f92672>.</span><span style=color:#111>m3</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#00a8c8>return</span> <span style=color:#111>x</span>
</span></span></code></pre></div><p>Explain all of this.</p><p>But now, we essentially built our entire network in own &ldquo;layer&rdquo; (actually we literally did, all =Layer=s are just =torch.Module=s) that does the job of all other layers acting together. To use it, we just:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>my_network</span> <span style=color:#f92672>=</span> <span style=color:#111>MyNetwork</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>input</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>randn</span><span style=color:#111>(</span><span style=color:#ae81ff>128</span><span style=color:#111>,</span> <span style=color:#ae81ff>20</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># function call syntax! Functions call from rigth to left!</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span> <span style=color:#f92672>=</span> <span style=color:#111>my_network</span><span style=color:#111>(</span><span style=color:#111>input</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>output</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>tensor([[-0.1694,  0.0095,  0.4306,  ...,  0.1580,  0.2644,  0.1509],
</span></span><span style=display:flex><span>        [-0.2346, -0.0269, -0.1191,  ...,  0.0229, -0.0819, -0.1452],
</span></span><span style=display:flex><span>        [-0.4871, -0.2868, -0.2488,  ...,  0.0637,  0.1832,  0.0619],
</span></span><span style=display:flex><span>        ...,
</span></span><span style=display:flex><span>        [-0.1323,  0.2531, -0.1086,  ...,  0.0975,  0.0426, -0.2092],
</span></span><span style=display:flex><span>        [-0.4765,  0.1441, -0.0520,  ...,  0.2364,  0.0253, -0.1914],
</span></span><span style=display:flex><span>        [-0.5044, -0.3263,  0.3102,  ...,  0.1938,  0.1427, -0.0587]],
</span></span><span style=display:flex><span>       grad_fn=&lt;AddmmBackward0&gt;)
</span></span></code></pre></div><p>But wait! What are the options you give to your optimizer? Surely you don&rsquo;t have to pass <code>my_network.m1.weight</code>, <code>my_network.m1.bias</code>, etc. etc. to the optimizer, right?</p><p>You don&rsquo;t. One of the things that the <code>super().__init__()</code> did was to register a special function to your network class that keeps track of everything to optimize for. So now, to ask the optimizer to update the entire network, you just have to write:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>optim</span> <span style=color:#f92672>=</span> <span style=color:#111>torch</span><span style=color:#f92672>.</span><span style=color:#111>optim</span><span style=color:#f92672>.</span><span style=color:#111>SGD</span><span style=color:#111>(</span><span style=color:#111>my_network</span><span style=color:#f92672>.</span><span style=color:#111>parameters</span><span style=color:#111>(),</span> <span style=color:#111>lr</span><span style=color:#f92672>=</span><span style=color:#ae81ff>1e-2</span><span style=color:#111>,</span> <span style=color:#111>momentum</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.9</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>optim</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>SGD (
</span></span><span style=display:flex><span>Parameter Group 0
</span></span><span style=display:flex><span>    dampening: 0
</span></span><span style=display:flex><span>    differentiable: False
</span></span><span style=display:flex><span>    foreach: None
</span></span><span style=display:flex><span>    lr: 0.01
</span></span><span style=display:flex><span>    maximize: False
</span></span><span style=display:flex><span>    momentum: 0.9
</span></span><span style=display:flex><span>    nesterov: False
</span></span><span style=display:flex><span>    weight_decay: 0
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>TODO make students recall original backprop example, backprope and step and zero_grad with this new optim.</p><p>Look! Optimizing an entire network works in the <em>exact same way</em> as optimizing two lone variables.</p><h2 id=putting-it-together>Putting it together</h2><p>TODO</p><ol><li>training loop (zero first, call model, get diff/loss, .backward(), .step())</li><li>best practices</li><li>saving and restoring models</li><li>GPU</li></ol></div></article></main></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
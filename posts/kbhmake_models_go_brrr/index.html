<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>Make Models Go Brrr: Model Parallel Whisper Training</title>
<meta name=description content="Happy Monday friends.
The deliverable of the week was to make the a ASR model for Batchalign. Essentially, most copies of Whisper is pretty bad at Language Sample Analysis (LSA), because they mostly don&rsquo;t work in terms trying to actually capture the things that people doing LSA want to capture (disfluencies, stuttering, etc.). OpenAI even acknowledged in the paper that they filtered out the disfluencies from their gold transcript to prevent Whisper from writing down too much of them."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>Make Models Go Brrr: Model Parallel Whisper Training</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p>Happy Monday friends.</p><p>The deliverable of the week was to make the a ASR model for Batchalign. Essentially, most copies of Whisper is pretty bad at Language Sample Analysis (LSA), because they mostly don&rsquo;t work in terms trying to actually capture the things that people doing LSA want to capture (disfluencies, stuttering, etc.). OpenAI even acknowledged in the paper that they filtered out the disfluencies from their gold transcript to prevent Whisper from writing down too much of them.</p><p>And so&mldr; We roll up our sleeves and do it ourselves.</p><h2 id=a-large-language-model>A <strong>Large</strong> Language Model</h2><p>I didn&rsquo;t want to perform Low-Rank Approximation (LoRA) to heavily when training this model. Folks fine tuning <a href=/posts/kbhllama/>LLaMA</a> will note that the preferred parameters were <a href=https://deci.ai/blog/fine-tune-llama-2-with-lora-for-question-answering/>essentially asked the user to make the model matricies Rank 8</a>, across the entire model.</p><p>When trying this in earlier experiments, we failed dramatically as the LoRA&rsquo;d model failed to converge when we hit any smaller rank below 10. However, if we tried to, say, do it above 10, I would OOM.</p><p>I will note: its not like we don&rsquo;t have compute. For this project, I fortunately am able to provision any number of V100 32GB as I see reasonable to train this model. Nevertheless, a lovey dovey parameter heavy 1.5 Billion parameter model is still a sight to behold (and cram into one such GPUs).</p><p>Hence, the most important impetus for making this work without aggressive LoRA and degraded performance is some kind of model parallel training scheme.</p><h2 id=one-model-multiple-cards>One Model, Multiple Cards</h2><figure><img src=/ox-hugo/2023-10-23_10-21-34_screenshot.png></figure><figure><img src=/ox-hugo/2023-10-23_10-21-40_screenshot.png></figure><p>Alr then.</p><p>After investigation, <a href=https://deepspeed.readthedocs.io/en/stable/zero3.html>DeepSpeed</a> seemed pretty promising for a few reasons. The third iteration of its algorithm (Zero-3) has three different main offerings:</p><ol><li>Model parameter sharding (sharding the weights of the model across devices)</li><li>Optimizer state sharding</li><li><strong>Model/Parameter state offload</strong></li></ol><p>The last one caught my eye. Essentially, as long as your chip has the ability to perform a single forward pass, it can train a model under Zero-3. This is because the system is designed, on request, to offload the weights of your model into CPU or NVMe if you want&mdash;and only pull it into the main device for the actual step of forward/backwards passes.</p><p>The thing about DeepSpeed is that its configured in a very hapazard way, and once you DeepSpeed onto your training script you can&rsquo;t really go back: it expects model parallel training, in the way you configured it, always, based on the contents to the training script.</p><p><a href=https://github.com/huggingface/accelerate>Huggingface Accelerate</a> to the rescue! The system is essentially a generic hypervisation framework. It is designed to accelerate model training using any framework you&rsquo;d like: CPU data parallel, GPU data parallel, DeepSpeed model parallel, and so on&mdash;with a single configuration file.</p><p>With minimal change to your <em>training script</em>, your actual acceleration scheme travels with a configuration file <strong>on device</strong>. Meaning, running the same script on different devices configured with Accelerate will use the best settings for that device; including the correct number of cards, accelerators, etc.</p><h2 id=pedal-to-the-metal>Pedal to the Metal</h2><p>As usual, despite how good all of this stuff sounds, getting it all to glue together was a hot mess.</p><h3 id=accelerate>Accelerate</h3><p>Let&rsquo;s start with Accelerate. The actual process of integrating Accelerate into your training script is pretty straightforward:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>accelerator</span> <span style=color:#f92672>=</span> <span style=color:#111>Accelerator</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>DEVICE</span> <span style=color:#f92672>=</span> <span style=color:#111>accelerator</span><span style=color:#f92672>.</span><span style=color:#111>device</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>optim</span><span style=color:#111>,</span> <span style=color:#111>dataloader</span><span style=color:#111>,</span> <span style=color:#111>val_dataloader</span> <span style=color:#f92672>=</span> <span style=color:#111>accelerator</span><span style=color:#f92672>.</span><span style=color:#111>prepare</span><span style=color:#111>(</span><span style=color:#111>model</span><span style=color:#111>,</span> <span style=color:#111>optim</span><span style=color:#111>,</span> <span style=color:#111>dataloader</span><span style=color:#111>,</span> <span style=color:#111>val_dataloader</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>and then, in your training loop, change</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>- loss.backward()
</span></span><span style=display:flex><span>+ accelerator.backward(loss)
</span></span></code></pre></div><p>and finally, whenever you need to access a value in CPU, change</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>- loss = torch.mean(loss.cpu())
</span></span><span style=display:flex><span>+ loss = torch.mean(accelerator.gather(loss))
</span></span></code></pre></div><p>That&rsquo;s honestly about it in terms of making accelerate work.</p><h3 id=deepspeed-shenanigans>DeepSpeed Shenanigans</h3><p>DeepSpeed is a great tool to accelerate model training, but the damned thing is so janky to actually get started because of various device integration issues.</p><p>There&rsquo;s this <a href=https://www.reddit.com/r/Oobabooga/comments/13etobg/using_deepspeed_requires_lots_of_manual_tweaking/>excellent thread</a> on Reddit with people winging about the various things that DeepSpeed is broken about. To actually get it to actually work on my end&mldr;</p><ol><li><em>deep breath. pray to deity of your choice, etc.</em> and Install Conda</li><li><code>pip install deepspeed</code></li><li><code>conda install openmpi</code></li><li><code>pip install mpi4py</code> (if this fails, <code>env LD_LIBRARY_PATH=/your/conda/lib/path pip install --no-cache-dir mpi4py</code>)</li></ol><p>If you now ran DeepSpeed on a model, it likely will crash on a local random assert statement. To fix this, get ready:</p><p>find <code>runtime/zero/partitioned_param_coordinator.py</code> wherever your DeepSpeed code is, and:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>- assert param.ds_status == ZeroParamStatus.AVAILABLE, param.ds_summary()
</span></span><span style=display:flex><span>+ # assert param.ds_status == ZeroParamStatus.AVAILABLE, param.ds_summary()
</span></span></code></pre></div><p>comment the damned assertion out. Yup.</p><p>Oh, also, this <a href=https://github.com/huggingface/trl/pull/687>https://github.com/huggingface/trl/pull/687</a> if you are running inference.</p><h3 id=accelerate-device-config>Accelerate Device Config</h3><p>And now, onto the device configuration. If you are most normal people, you can just run:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>accelerate config
</span></span></code></pre></div><p>answer the questions, and be done for configuring that device. However, as I was training on a SLURM device, I had no access to a tty. Hence, I had to configure the Accelerate device configuration myself.</p><p>To glue Accelerate and Deepspeed together, here was the config.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>compute_environment</span><span style=color:#111>:</span> <span style=color:#ae81ff>LOCAL_MACHINE</span>
</span></span><span style=display:flex><span><span style=color:#f92672>debug</span><span style=color:#111>:</span> <span style=color:#00a8c8>false</span>
</span></span><span style=display:flex><span><span style=color:#f92672>deepspeed_config</span><span style=color:#111>:</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>gradient_accumulation_steps</span><span style=color:#111>:</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>offload_optimizer_device</span><span style=color:#111>:</span> <span style=color:#ae81ff>none</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>offload_param_device</span><span style=color:#111>:</span> <span style=color:#ae81ff>cpu</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>zero3_init_flag</span><span style=color:#111>:</span> <span style=color:#00a8c8>true</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>zero_stage</span><span style=color:#111>:</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#f92672>distributed_type</span><span style=color:#111>:</span> <span style=color:#ae81ff>DEEPSPEED</span>
</span></span><span style=display:flex><span><span style=color:#f92672>fsdp_config</span><span style=color:#111>:</span> {}
</span></span><span style=display:flex><span><span style=color:#f92672>downcast_bf16</span><span style=color:#111>:</span> <span style=color:#d88200>&#39;no&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>machine_rank</span><span style=color:#111>:</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#f92672>mixed_precision</span><span style=color:#111>:</span> <span style=color:#d88200>&#39;no&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>num_machines</span><span style=color:#111>:</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>num_processes</span><span style=color:#111>:</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#f92672>use_cpu</span><span style=color:#111>:</span> <span style=color:#00a8c8>false</span>
</span></span></code></pre></div><p>Here are the highlights:</p><p><code>mixed_precision: 'no'</code>: FP16 doesn&rsquo;t work if you do your own tensor creation within the train loop as I did though the Whisper models. Your DataLoader passed to your accelerator at the beginning of the script must return the <strong>exact</strong> tensors you put into the model if you want FP16.</p><p><code>offload_optimizer_device: none</code>: offloading optimizer requires you to compile the PyTorch extension <code>adam_cpu</code> from DeepSpeed. I never got it to work on the training rig because it required CUDA headers (why? how? why is <code>adam_cpu</code> CUDA? no clue). Notably, <strong>optimizer SHARDING</strong> across GPUs still work, because that has nothing to do with offload.</p><p><code>zero_stage: 3</code>: stage 1 is state sharding, 2 is optimizer sharding, 3 is optimizer AND parameter sharding.</p><p><code>num_processes: 3</code>: for GPUs, <code>num_processes</code> is <strong>the number of GPUs</strong> Accelerate/DeepSpeed should use.</p><h3 id=friggin-lora>Friggin LoRA</h3><p>In the sprit of not wasting too many monies, I still conceded and used LoRA. This was a fairly straightforward setup through Huggingface PEFT.</p><p>Here was my config:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>peft_config</span> <span style=color:#f92672>=</span> <span style=color:#111>LoraConfig</span><span style=color:#111>(</span><span style=color:#111>inference_mode</span><span style=color:#f92672>=</span><span style=color:#00a8c8>False</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                         <span style=color:#111>r</span><span style=color:#f92672>=</span><span style=color:#ae81ff>16</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                         <span style=color:#111>target_modules</span><span style=color:#f92672>=</span><span style=color:#111>[</span><span style=color:#d88200>&#34;q_proj&#34;</span><span style=color:#111>,</span> <span style=color:#d88200>&#34;v_proj&#34;</span><span style=color:#111>,</span> <span style=color:#d88200>&#34;out_proj&#34;</span><span style=color:#111>],</span>
</span></span><span style=display:flex><span>                         <span style=color:#111>lora_alpha</span><span style=color:#f92672>=</span><span style=color:#ae81ff>32</span><span style=color:#111>,</span>
</span></span><span style=display:flex><span>                         <span style=color:#111>lora_dropout</span><span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>and the integration:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>model = WhisperForConditionalGeneration.from_pretrained(f&#34;{MODEL}&#34;)
</span></span><span style=display:flex><span>+ model = get_peft_model(model, peft_config)
</span></span></code></pre></div><p>Simple as that. One protip: call <code>model.train()</code>; otherwise you will be hit with:</p><pre tabindex=0><code class=language-nil data-lang=nil>  File &#34;/jet/home/hliuk/.conda/envs/chat-whisper/lib/python3.10/site-packages/torch/nn/modules/conv.py&#34;, line 309, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: weight should have at least three dimensions
</code></pre><p>presumably because of some conflict with <code>inference_mode</code> setting the wrong <code>.forward()</code> paths.</p><p>On the machine, <code>merge_and_unload</code> never worked. Instead, I downloaded the LoRA weights (instead of the merged full weights) and then called that on my local machine.</p><p>Two highlights from the LoRA config:</p><p><code>r=16</code>: we set the rank of the matrix into <code>16</code>, because anything lower causes the model to stop converging. This still ended up needing 3 GPUs to actually cram fit.</p><p><code>lora_alpha=32</code>: I saw somewhere that the LoRA weight scaling factor, which is <code>lora_alpha/r</code>, should always be larger that \(1\). Your mileage may vary.</p><p><code>["q_proj", "v_proj", "out_proj"]</code>: it seems like many people are not a fan of LoRAing the key matricies&mdash;why? I don&rsquo;t know. I&rsquo;m following that convention here.</p><h2 id=and-so-dot-dot-dot>And so&mldr;</h2><p>Two days, and much wandb later, we&rsquo;ve got a model!</p><p><a href=https://huggingface.co/talkbank/CHATWhisper-en-large-v1>Check it out!</a></p><figure><img src=/ox-hugo/2023-10-23_13-16-01_screenshot.png></figure><p>We could&rsquo;ve pushed the GPU up a little by setting LoRA rank higher, but I found that if the memory is sitting at anything above a \(80\%\) ever, the system will eventually OOM.</p></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
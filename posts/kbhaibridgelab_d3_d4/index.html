<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.7.2/css/all.min.css integrity="sha512-Evv84Mr4kqVGRNSgIGL/F/aIDqQb7xQ2vcrdIwxfjThSH8CSR7PBEakCr51Ck+w+/U6swU2Im1vVX0SVk9ABhg==" crossorigin=anonymous referrerpolicy=no-referrer><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin=anonymous></script><meta name=viewport content="width=device-width,initial-scale=1"><script src=https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4></script><link rel=preconnect href=https://fonts.bunny.net><link href="https://fonts.bunny.net/css?family=ibm-plex-sans:300,400,400i,500,500i,600,700,700i" rel=stylesheet><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.MathJax={loader:{load:["[tex]/physics"]},tex:{packages:{"[+]":["physics"]}}}</script><title>AIBridgeLab D3/D4</title>
<meta name=description content="Woah! We talked about a lot of different ways of doing classification today! Let&rsquo;s see what we can do about this for the Iris dataset!
Iris Dataset
Let&rsquo;s load the Iris dataset! Begin by importing the load_iris tool from sklearn. This is an easy loader scheme for the iris dataset.
from sklearn.datasets import load_iris
Then, we simply execute the following to load the data.
x,y = load_iris(return_X_y=True)
We use the return_X_y argument here so that, instead of dumping a large CSV, we get the neat-cleaned input and output values."><meta name=author content="Houjun Liu"><link rel=stylesheet href=/css/all.css><link rel=stylesheet href=/css/page.css><link rel=stylesheet href=/css/syntax.css><link rel=stylesheet href=/css/typography.css><link rel=stylesheet href=/css/components.css></head><body><div class="center-clearfix p-10" style="max-width:1200px;margin:0 auto"><header class=pb-4><span class="w-full flex justify-between flex-wrap"><div style=font-weight:600;font-size:15px><a style=border:0;cursor:pointer href=/><img src=/images/Logo_Transparent.png style=width:17px;display:inline-block;margin-right:8px;transform:translateY(-2.7px)></a>AIBridgeLab D3/D4</div><span style="float:right;display:flex;align-items:center;margin-top:5px;width:100%;max-width:400px;background:#fff;border-radius:2px;border:1px solid var(--gray-2);position:relative"><i class="fa-solid fa-magnifying-glass" style=font-size:12px;color:var(--yellow-fg);margin-right:7px;margin-left:10px></i><div style=width:100%><input id=search-query-inline name=s type=text autocomplete=off placeholder="Search Knowledgebase" enterkeyhint=search></div><div id=search-result-inline style=display:none></div><script id=search-result-template type=text/x-js-template data-template=searchresult>
    <a href="${link}" class="search-link">
    <div class="search-result-item" id="search-result-item-${key}">
        <div class="search-result-title">${title}</div>
        <div class="search-result-summary">${snippet}</div>
    </div>
    </a>
</script><script src=https://code.jquery.com/jquery-3.3.1.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.0/fuse.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js></script><script type=module>
    import { create, insertMultiple, search } from 'https://cdn.jsdelivr.net/npm/@orama/orama@latest/+esm'
    let template = $('script[data-template="searchresult"]').text().split(/\$\{(.+?)\}/g);

    function render(props) {
        return function(tok, i) { return (i % 2) ? props[tok] : tok; };
    }

    const db = create({
        schema: {
            title: 'string',
            content: 'string',
        },
    });
    $.get("https://www.jemoka.com/index.json", function(data, status){
        insertMultiple(db, data.map(x =>
            ({title: x.title, content: x.contents, link: x.permalink})));
    });
    $("#search-result-inline").attr("tabindex", "-1");
    $(document).on("click", function (e) {
    if (!$(e.target).closest("#search-query-inline, #search-result-inline").length) {
        $("#search-result-inline").hide();
    }
    });
    
    
    
    
    
    

    $("#search-query-inline").on("focus keyup", function () {
        let value = $(this).val();
        if (value.trim() == "") {
            $("#search-result-inline").html("");
            $("#search-result-inline").hide();
            return;
        }
        $("#search-result-inline").show();
        let results = search(db, {mode: "fulltext", term: value});
        let contents = results.hits.map(x => template.map(render({
            title: x.document.title,
            snippet: x.document.content.slice(0, 52),
            key: x.id,
            link: x.document.link
        })).join(''));
        $("#search-result-inline").html(contents.join("\n"));
        
    });
    if (/iPhone|iPad|iPod/i.test(navigator.userAgent)) {
        $('#search-query-inline').on('focus', function(){
            
            $(this).data('fontSize', $(this).css('font-size')).css('font-size', '16px');
        }).on('blur', function(){
            
            $(this).css('font-size', $(this).data('fontSize'));
        });
    }

</script></span></span><div class=w-full style="padding-bottom:4px;border-bottom:1px solid var(--gray-1);margin-top:1px"></div></header><aside id=tocbox><div id=heading style=padding-left:40px;font-weight:600;font-size:11px;color:var(--gray-3)>contents</div><div id=toc></div></aside><main><article><div style=max-width:700px><p>Woah! We talked about a lot of different ways of doing classification today! Let&rsquo;s see what we can do about this for the Iris dataset!</p><h2 id=iris-dataset>Iris Dataset</h2><p>Let&rsquo;s load the Iris dataset! Begin by importing the <code>load_iris</code> tool from <code>sklearn</code>. This is an easy loader scheme for the iris dataset.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.datasets</span> <span style=color:#f92672>import</span> <span style=color:#111>load_iris</span>
</span></span></code></pre></div><p>Then, we simply execute the following to load the data.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span> <span style=color:#f92672>=</span> <span style=color:#111>load_iris</span><span style=color:#111>(</span><span style=color:#111>return_X_y</span><span style=color:#f92672>=</span><span style=color:#00a8c8>True</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>We use the <code>return_X_y</code> argument here so that, instead of dumping a large <code>CSV</code>, we get the neat-cleaned input and output values.</p><p>A reminder that there is three possible flowers that we can sort by.</p><h2 id=decision-trees>Decision Trees</h2><p>Scikit learn has great facilities for using decision trees for classification! Let&rsquo;s use some of them by fitting to the Iris dataset.</p><p>Let us begin by importing the SciKit learn tree system:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.tree</span> <span style=color:#f92672>import</span> <span style=color:#111>DecisionTreeClassifier</span>
</span></span></code></pre></div><p>We will fit and instantiate this classifier and fit it to the data exactly!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>DecisionTreeClassifier</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>fit</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>One cool thing about decision trees is that we can actually see what its <em>doing!</em> by looking at the series of splits and decisions. This is a function provided by tree too.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># We first import the plotting utility from matplotlib</span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> <span style=color:#111>matplotlib.pyplot</span> <span style=color:#00a8c8>as</span> <span style=color:#111>plt</span>
</span></span><span style=display:flex><span><span style=color:#75715e># as well as the tree plotting tool</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.tree</span> <span style=color:#f92672>import</span> <span style=color:#111>plot_tree</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># We call the tree plot tool, which puts it on teh matplotlib graph for side effects</span>
</span></span><span style=display:flex><span><span style=color:#111>plot_tree</span><span style=color:#111>(</span><span style=color:#111>clf</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#75715e># And we save the figure</span>
</span></span><span style=display:flex><span><span style=color:#111>plt</span><span style=color:#f92672>.</span><span style=color:#111>savefig</span><span style=color:#111>(</span><span style=color:#d88200>&#34;tree.png&#34;</span><span style=color:#111>)</span>
</span></span></code></pre></div><figure><img src=/ox-hugo/tree.png></figure><p>Cool! As you can see, by the end of the entire graph, the gini impurity of each node has been sorted to 0.</p><p>Apparently, if the third feature (pedal length) is smaller that 2.45, it is definitely the first type of flower!</p><figure><img src=/ox-hugo/2022-06-23_11-46-34_screenshot.png></figure><p>Can you explain the rest of the divisions?</p><p>There are some arguments available in <code>.fit</code> of a <code>DecisionTreeClassifier</code> which controls for when splitting ends; for instance, <code>max_depth</code> controls the maximum depth by which the tree can go.</p><h2 id=extra-addition-random-forests-dot>Extra Addition! Random Forests.</h2><p>If you recall, we make the initial splitting decisions fairly randomly, and simply select the one with the lowest Ginni impurity. Of course, this makes the selection of the initial sets of splits very important.</p><p>What if, instead of needing to make a decision about that now, we can just deal with it later? Well, that&rsquo;s where the addition of Random Forests come in.</p><p>As the name suggests, instead of having one great tree that does a &ldquo;pretty good&rdquo; job, we can have a lot of trees acting in <em>ensemble!</em> We can randomly start a bunch of random trees, and pick the selection that most would correspond with.</p><p>Random forests come from the ensemble package from <code>sklearn</code>; we can use it fairly simply:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.ensemble</span> <span style=color:#f92672>import</span> <span style=color:#111>RandomForestClassifier</span>
</span></span><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>RandomForestClassifier</span><span style=color:#111>()</span>
</span></span></code></pre></div><p>Wonderful! I bet you can guess what the syntax is. Instead of fitting on the whole dataset, though, we will fit on the first 145 items.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>fit</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>[:</span><span style=color:#f92672>-</span><span style=color:#ae81ff>5</span><span style=color:#111>],</span><span style=color:#111>y</span><span style=color:#111>[:</span><span style=color:#f92672>-</span><span style=color:#ae81ff>5</span><span style=color:#111>])</span>
</span></span></code></pre></div><p>We can go ahead and run predict on some samples, just to see how it does on data it has not already seen before!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>score</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>5</span><span style=color:#111>:],</span> <span style=color:#111>y</span><span style=color:#111>[</span><span style=color:#f92672>-</span><span style=color:#ae81ff>5</span><span style=color:#111>:])</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>1.0
</span></span></code></pre></div><p>As you can see, it still does pretty well!</p><h2 id=svm>SVM</h2><p>Let&rsquo;s put another classification technique we learned today to use! Support Vector Machines. The entire syntax to manipulate support vector machines is very simple; at this point, you can probably guess it in yours sleep :)</p><p>Let&rsquo;s import a SVM:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn</span> <span style=color:#f92672>import</span> <span style=color:#111>svm</span>
</span></span></code></pre></div><p>Great. Now, we will instantiate it and fit it onto the data. <code>SVC</code> is the support-vector machine classifier.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>svm</span><span style=color:#f92672>.</span><span style=color:#111>SVC</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>fit</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>Excellent, now, let&rsquo;s score our predictions:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>score</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>0.9733333333333334
</span></span></code></pre></div><p>As you can see, our data is not entirely linear! Fitting our entire dataset onto a linear SVM didn&rsquo;t score perfectly, which means that the model is not complex enough to support our problem.</p><p>Scikit&rsquo;s support vector machine supports lots of nonlinearity function; this is set by the argument <code>kernel</code>. For instance, if we wanted a nonlinear, exponential function kernel (where nonlinear function \(f(x,x&rsquo;)= e^{-\gamma||\big&lt;x,x&rsquo;\big>||^2}\)), we can say:</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>svm</span><span style=color:#f92672>.</span><span style=color:#111>SVC</span><span style=color:#111>(</span><span style=color:#111>kernel</span><span style=color:#f92672>=</span><span style=color:#d88200>&#34;rbf&#34;</span><span style=color:#111>)</span>
</span></span><span style=display:flex><span><span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>fit</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>score</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>0.9733333333333334
</span></span></code></pre></div><p>Looks like our results are fairly similar, though.</p><h2 id=naive-bayes>Naive Bayes</h2><p>One last one! Its Bayes time. Let&rsquo;s first take a look at how an Naive Bayes implementation can be done via Scikit learn.</p><p>One of the things that the Scikit Learn Naive Bayes estimator does differently than the one that we learned via probabilities is that it assumes that&mdash;instead of a uniform distribution (and therefore &ldquo;chance of occurrence&rdquo; is just occurrence divided by count), our samples are normally distributed. Therefore, we have that</p><p>\begin{equation}
P(x_i | y) = \frac{1}{\sqrt{2\pi{\sigma^2}_y}}e^{\left(-\frac{(x_i-\mu_y)^2}{2{\sigma^2}_y}\right)}
\end{equation}</p><p>We can instantiate such a model with the same exact syntax.</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> <span style=color:#111>sklearn.naive_bayes</span> <span style=color:#f92672>import</span> <span style=color:#111>GaussianNB</span>
</span></span><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>GaussianNB</span><span style=color:#111>()</span>
</span></span><span style=display:flex><span><span style=color:#111>clf</span> <span style=color:#f92672>=</span> <span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>fit</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span><span style=color:#111>)</span>
</span></span></code></pre></div><p>Let&rsquo;s see how it does!</p><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#111>clf</span><span style=color:#f92672>.</span><span style=color:#111>score</span><span style=color:#111>(</span><span style=color:#111>x</span><span style=color:#111>,</span><span style=color:#111>y</span><span style=color:#111>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>0.96
</span></span></code></pre></div><p>Same thing as before, it seems simple probabilities can&rsquo;t model our relationship super well. However, this is still a fairly accurate and powerful classifier.</p><h2 id=now-you-try>Now you try!</h2><ul><li>Try all three classifiers on the Wine dataset for red-white divide! Which one does better on generalizing to data you haven&rsquo;t seen before?</li><li>Explain the results of the decision trees trained on the Wine data by plotting it. Is there anything interesting that the tree used as a heuristic that came up?</li><li>The <em>probabilistic</em>, uniform Naive-Bayes is fairly simple to implement write if we are using the traditional version of the Bayes theorem. Can you use Pandas to implement one yourself?</li></ul></div></article></main><footer style=margin-top:20px><p id=footer style=font-size:8px;color:var(--gray-3)>&copy; 2019-2025 Houjun Liu. Licensed CC BY-NC-SA 4.0.</p></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.32.2/tocbot.min.js></script><script>tocbot.init({tocSelector:"#toc",contentSelector:".center-clearfix",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></body></html>
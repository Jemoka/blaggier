+++
title = "Language Agents with Karthik"
author = ["Houjun Liu"]
draft = false
+++

## Transitions {#transitions}

1.  Transition first from rule based learning to statistical learning
2.  Rise of semantic parsing: statistical models of parsing
3.  Then, moving from semantic parsing to large models---putting decision making and language modeling into the same bubble


## Importance of LLMs {#importance-of-llms}

-   They are simply better at understanding language inputs
-   They can generate structured information (i.e. not just human language, JSONs, etc.)
-   They can perform natural language "reasoning"---not just generate

(and natural language generation, abv)

-   1+3 gives you chain of thought reasoning
-   1+2 gives CALM, SayCan, and other types of RL text parsing in order to do stuff with robotics
-   all three gives ReAct


## ReAct {#react}

See [ReAct]({{< relref "KBhreact.md#react" >}})


## Problem: agents are not robust at all {#problem-agents-are-not-robust-at-all}

<https://github.com/ryoungj/ToolEmu>


## Key Challenges {#key-challenges}

See [History of Agents and Their Challenges]({{< relref "KBhinteractive_agent.md#challenge-of-making-agents" >}})

+++
title = "SU-CS238 OCT192023"
author = ["Houjun Liu"]
draft = false
+++

## Key Sequence {#key-sequence}


## Notation {#notation}


## New Concepts {#new-concepts}

-   [Markov Decision Process]({{< relref "KBhmarkov_decision_process.md" >}})
    -   [value iteration]({{< relref "KBhvalue_iteration.md" >}})
-   [Bellman Residual]({{< relref "KBhvalue_iteration.md#bellman-residual" >}})
-   for [continuous]({{< relref "KBhuniqueness_and_existance.md#continuity" >}}) state spaces: [Approximate Value Function]({{< relref "KBhapproximate_value_function.md" >}})
    -   use [global approximation]({{< relref "KBhapproximate_value_function.md#global-approximation" >}}) or [local approximation]({{< relref "KBhapproximate_value_function.md#local-approximation" >}}) methods


## Important Results / Claims {#important-results-claims}

-   policy and utility
    -   creating a good [utility function]({{< relref "KBhutility_function.md" >}}) / [policy]({{< relref "KBhpolicy.md" >}}) from instantaneous rewards: either [policy evaluation]({{< relref "KBhpolicy_evaluation.md" >}}) or [value iteration]({{< relref "KBhvalue_iteration.md" >}})
    -   creating a [policy]({{< relref "KBhpolicy.md" >}}) from a [utility function]({{< relref "KBhutility_function.md" >}}): [value-function policy]({{< relref "KBhaction_value_function.md#value-function-policy" >}}) ("choose the policy that takes the best valued action")
    -   calculating the [utility function]({{< relref "KBhutility_theory.md" >}}) a [policy]({{< relref "KBhpolicy.md" >}}) currently uses: use [policy evaluation]({{< relref "KBhpolicy_evaluation.md" >}})
-   [kernel smoothing]({{< relref "KBhkernel_smoothing.md" >}})
-   [value iteration, in practice]({{< relref "KBhvalue_iteration_in_practice.md" >}})


## Questions {#questions}


## Interesting Factoids {#interesting-factoids}

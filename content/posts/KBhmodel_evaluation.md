+++
title = "Model Evaluation"
author = ["Houjun Liu"]
draft = false
+++

## Extrinsic Evaluation {#extrinsic-evaluation}

[Extrinsic Evaluation](#extrinsic-evaluation), also known as [In-Vivo Evaluation](#extrinsic-evaluation), focuses on benchmarking two language models in terms of their differing performance on a test task.


## Intrinsic Evaluation {#intrinsic-evaluation}

[In-Vitro Evaluation](#intrinsic-evaluation) or [Intrinsic Evaluation](#intrinsic-evaluation) focuses on evaluating the language models' performance at, well, language modeling.

Typically, we use [perplexity]({{< relref "KBhperplexity.md" >}}).

-   directly measure language model performance
-   doesn't necessarily correspond with real applications

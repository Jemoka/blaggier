+++
title = "SU-CS238 OCT052023"
author = ["Houjun Liu"]
draft = false
+++

## Key Sequence {#key-sequence}


## Notation {#notation}


## New Concepts {#new-concepts}

-   [inference]({{< relref "KBhinference.md" >}})
    -   [Inference for Gaussian Models]({{< relref "KBhinference_for_gaussian_models.md" >}})
-   [approximate inference]({{< relref "KBhapproximate_inference.md" >}})
    -   [Direct Sampling]({{< relref "KBhdirect_sampling.md" >}})
    -   [Likelihood Weighted Sampling]({{< relref "KBhdirect_sampling.md#likelihood-weighted-sampling" >}})
-   [parameter learning]({{< relref "KBhparameter_learning.md" >}})
    -   [Maximum Likelihood Parameter Learning]({{< relref "KBhmaximum_likelihood_parameter_learning.md" >}})
    -   [Baysian Parameter Learning]({{< relref "KBhbaysian_parameter_learning.md" >}})
        -   [Dirichlet Distribution]({{< relref "KBhbaysian_parameter_learning.md#dirichlet-distribution" >}})


## Important Results / Claims {#important-results-claims}

-   "there is usually a tradeoff between the computational time you are willing to devote, and th
-   [Bayesian Learning on Binary Distributions]({{< relref "KBhbaysian_parameter_learning.md#bayesian-parameter-learning-on-binary-distributions" >}})


## Questions {#questions}

-   for [Likelihood Weighted Sampling]({{< relref "KBhdirect_sampling.md#likelihood-weighted-sampling" >}}), where do the conditional probability values come from


## Interesting Factoids {#interesting-factoids}
